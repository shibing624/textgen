{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练领域模型--医疗模型\n",
    "\n",
    "分三阶段：\n",
    "\n",
    "- 第一阶段：PT(continue PreTraining)增量预训练，在海量领域文档数据上二次预训练LLaMA模型，如有需要可以扩充领域词表，比如医疗领域词表；\n",
    "- 第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在第一阶段的预训练模型基础上做指令精调；\n",
    "- 第三阶段：RM(Reward Model)奖励模型，和基于人类反馈的强化学习(RLHF), 此本部分见[shibing624/MedicalGPT](https://github.com/shibing624/MedicalGPT)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PT(continue PreTraining)增量预训练\n",
    "\n",
    "使用百科类文档类数据集，用来在领域数据集上增量预训练或二次预训练，期望能把领域知识注入给模型，以医疗领域为例，希望增量预训练，能让模型理解感冒的症状、病因、治疗药品、治疗方法、药品疗效等知识，便于后续的SFT监督微调能激活这些内在知识。\n",
    "\n",
    "这里说明一点，像GPT3、LLaMA这样的大模型理论上是可以从增量预训练中获益，但增量预训练需要满足两个要求：1）高质量的预训练样本；2）较大的计算资源，显存要求高，即使是用LoRA技术，也要满足block_size=1024或2048长度的文本加载到显存中。\n",
    "\n",
    "其次，如果你的项目用到的数据是模型预训练中已经使用了的，如维基百科、ArXiv等LLaMA模型预训练用了的，则这些数据是没有必要再喂给LLaMA增量预训练，而且预训练样本的质量如果不够高，也可能会损害原模型的生成能力。\n",
    "\n",
    "tips：PT阶段是可选项，请慎重处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/shibing624/textgen.git\n",
    "%cd textgen\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd examples/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from loguru import logger\n",
    "\n",
    "sys.path.append('../..')\n",
    "from textgen import LlamaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Loading data...')\n",
    "model_args = {\n",
    "    \"is_pretraining\": True,\n",
    "    \"block_size\": 1024,\n",
    "    \"use_peft\": True,\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"eval_batch_size\": 2,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"output_dir\": './outputs-pretraining-medical/',\n",
    "    \"resume_from_checkpoint\": './outputs-pretraining-medical/',\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 100,\n",
    "}\n",
    "# 此处以llama-plus为例，效果较好的中文模型如下：\n",
    "# 7b: minlik/chinese-llama-plus-7b-merged\n",
    "# 13b: shibing624/chinese-llama-plus-13b-hf\n",
    "model = LlamaModel('llama', 'shibing624/chinese-llama-plus-13b-hf', args=model_args)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([\"给出三个保持健康的秘诀。\"]))\n",
    "print(model.predict([\"乙肝和丙肝的区别\"]))\n",
    "print(model.predict([\"什么补肾最好\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pt_data(data, train_limit_size=500):\n",
    "    if data.endswith('.json') or data.endswith('.jsonl'):\n",
    "        dataset = load_dataset(\"json\", data_files=data)\n",
    "    elif os.path.isdir(data):\n",
    "        dataset = load_from_disk(data)\n",
    "    else:\n",
    "        dataset = load_dataset(data, 'pretrain')\n",
    "    logger.debug(dataset)\n",
    "\n",
    "    if train_limit_size > 0:\n",
    "        dataset_sample = dataset[\"train\"].shuffle(seed=42).select(range(train_limit_size))\n",
    "    else:\n",
    "        dataset_sample = dataset[\"train\"].shuffle(seed=42)\n",
    "    all_datasets = dataset_sample\n",
    "    logger.debug(f\"all_datasets size:{len(all_datasets)}, first line: {next(iter(all_datasets))}\")\n",
    "    data = []\n",
    "    for example in all_datasets:\n",
    "        data.append(example['text'])\n",
    "    logger.info(f\"total data size:{len(data)}, head: {data[:3]}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_pt_data('shibing624/medical', 500)\n",
    "logger.debug(f'train_data, size: {len(train_data)}, head top3: {train_data[:3]}')\n",
    "train_df = pd.DataFrame(train_data, columns=[\"text\"])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = train_df[:10]\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[10:]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练啦，我本地测试了下，block_size=1024, batch_size=2, peft_type=lora，model=llama-13b, 开fp16, 显存占用30G。\n",
    "\n",
    "- 如果你的显存不足，可以改小batch_size=1, block_size=512（影响训练的上下文最大长度）,peft_type=adalora;\n",
    "- 如果你的显存更大，可以改大block_size=2048, 此为llama原始预训练长度，不能更大啦；调大batch_size。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df, eval_data=eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([\"给出三个保持健康的秘诀。\"]))\n",
    "print(model.predict([\"乙肝和丙肝的区别\"]))\n",
    "print(model.predict([\"什么补肾最好\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下脚本是用lora训练时需要执行的，用来把lora权重merge到base model中，每一阶段都需要执行此merge操作。\n",
    "\n",
    "合并lora模型的权重参数到base model，\n",
    "\n",
    "细节可以参考合并脚本：textgen/llama/merge_peft_adapter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "output_dir = \"merged_pt\"\n",
    "base_model = model.model.merge_and_unload()\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.save_pretrained(output_dir)\n",
    "LlamaForCausalLM.save_pretrained(base_model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls merged_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT(Supervised Fine-tuning)有监督微调\n",
    "\n",
    "构造指令数据集，在第一阶段的预训练模型基础上做指令精调。\n",
    "\n",
    "如果是LoRA的增量预训练，需要先把LoRA权重merge到base model, 使用merge脚本完成合并操作，再SFT；\n",
    "如果没有做增量预训练，则直接在底座模型上SFT就可以。\n",
    "\n",
    "\n",
    "<img src=../../docs/rlhf.png height=500 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于llama-13b-plus底座模型或者merge_pt的底座模型，SFT监督训练生成llama-13b-sft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "def load_sft_data(debug=True):\n",
    "    raw_dataset = load_dataset('shibing624/medical', 'finetune')\n",
    "    logger.debug(raw_dataset)\n",
    "\n",
    "    all_datasets = []\n",
    "    if debug:\n",
    "        all_datasets.append(raw_dataset[\"train\"].shuffle().select(range(1000)))\n",
    "    else:\n",
    "        all_datasets.append(raw_dataset[\"train\"])\n",
    "    # sft 阶段，除了加载医疗训练集外，为了避免模型灾难遗忘，需要强化其正常对话能力，此处加载中英文alpaca、中文belle数据集\n",
    "    dataset_names = [\"shibing624/alpaca-zh\", \"yahma/alpaca-cleaned\", \"BelleGroup/train_0.5M_CN\", \"BelleGroup/generated_chat_0.4M\"]\n",
    "    for name in dataset_names:\n",
    "        raw_dataset = load_dataset(name)\n",
    "        ds = raw_dataset[\"train\"]\n",
    "        if debug:\n",
    "            ds = ds.shuffle().select(range(1000))\n",
    "        all_datasets.append(ds)\n",
    "    all_datasets = concatenate_datasets(all_datasets)\n",
    "    data = []\n",
    "    logger.debug(f\"all_datasets size:{len(all_datasets)}, first line: {next(iter(all_datasets))}\")\n",
    "    for example in all_datasets:\n",
    "        instruction, intput_str, output_str = example[\"instruction\"], example[\"input\"], example[\"output\"]\n",
    "        data.append([instruction, intput_str, output_str])\n",
    "    logger.info(f\"total data size:{len(data)}, head: {data[:3]}\")\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'merged_pt' if os.path.exists('merged_pt') else 'shibing624/chinese-llama-plus-13b-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune Llama model\n",
    "model_args = {\n",
    "    \"use_peft\": True,\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"max_seq_length\": 256,\n",
    "    \"max_length\": 256,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"num_train_epochs\": 0.5,\n",
    "    \"is_train_on_prompt\": False,\n",
    "    \"output_dir\": './outputs-sft/',\n",
    "    \"resume_from_checkpoint\": './outputs-llama-13b-sft/',\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 500,\n",
    "}\n",
    "model = LlamaModel('llama', model_name, args=model_args)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([\"给出三个保持健康的秘诀。\"]))\n",
    "print(model.predict([\"乙肝和丙肝的区别\"]))\n",
    "print(model.predict([\"什么补肾最好\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_sft_data(debug=True)\n",
    "logger.debug('train_data: {}'.format(train_data[:10]))\n",
    "train_df = pd.DataFrame(train_data, columns=[\"instruction\", \"input\", \"output\"])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = train_df[:10]\n",
    "train_df = train_df[10:]\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df, eval_data=eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction):\n",
    "    return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\\n\"\"\"\n",
    "\n",
    "print(model.predict([generate_prompt(\"给出三个保持健康的秘诀。\")]))\n",
    "print(model.predict([generate_prompt(\"乙肝和丙肝的区别\")]))\n",
    "print(model.predict([\"什么补肾最好\"]))\n",
    "print(model.predict([generate_prompt(\"防中风还可吃什么药？\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls outputs-llama-13b-sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把lora权重merge到base model中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"merged_sft\"\n",
    "base_model = model.model.merge_and_unload()\n",
    "model.tokenizer.save_pretrained(output_dir)\n",
    "LlamaForCausalLM.save_pretrained(base_model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls merged_sft"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意：使用lora训练，每一步的lora权重都要合并到base模型中，再进行下一步。"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
