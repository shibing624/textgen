[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/textgen/blob/main/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/textgen/blob/main/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/textgen/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) 

<div align="center">
  <a href="https://github.com/shibing624/textgen">
    <img src="https://github.com/shibing624/textgen/blob/main/docs/logo.svg" alt="Logo">
  </a>
</div>

-----------------

# TextGen: Implementation of Text Generation models
[![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
[![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)

## ğŸ“– Introduction

**TextGen** implements a variety of text generation models, including: LLaMA, ChatGLM, UDA, GPT2, Seq2Seq, BART, T5, SongNet and other models, out of the box.

## ğŸ˜Š Feature

- [ChatGLM](textgen/chatglm): This project implements the LoRA fine-tuning training and prediction of the ChatGLM-6B model based on PyTorch, which can be used for text generation tasks such as sentence error correction and dialogue
- [LLaMA](textgen/llama): This project implements the LLaMA model LoRA fine-tuning training and prediction based on PyTorch, which can be used for dialogue generation tasks and domain fine-tuning training
- [BLOOM](textgen/bloom): This project implements the BLOOM model LoRA fine-tuning training and prediction based on PyTorch, which can be used for dialogue generation tasks and domain fine-tuning training
- [UDA/EDA](textgen/augment/word_level_augment.py): This project implements UDA (non-core word replacement), EDA and Back Translation (back translation) algorithms, and replaces some unimportant words in sentences based on TF-IDF For synonyms, random word insertion, deletion, replacement, etc., generate new text and realize text amplification
- [Seq2Seq](textgen/seq2seq): This project implements the training and prediction of Seq2Seq, ConvSeq2Seq, and BART models based on PyTorch, which can be used for text generation tasks such as text translation, dialogue generation, and abstract generation
- [T5](textgen/t5): This project implements T5 and CopyT5 model training and prediction based on PyTorch, which can be used for text generation tasks such as text translation, dialogue generation, couplet generation, and copywriting
- [GPT2](textgen/language_modeling): This project implements GTP2 model training and prediction based on PyTorch, which can be used for text generation tasks such as article generation and couplet generation
- [SongNet](textgen/language_modeling/songnet_model.py): This project implements SongNet model training and prediction based on PyTorch, which can be used for text generation tasks such as poems and lyrics in standardized formats
- [TGLS](textgen/unsup_generation): This project implements the [TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5) unsupervised similar text generation model, which is a "first The text generation method of "learning after searching" learns the candidate set repeatedly, and the final model can generate high-quality similar text similar to the candidate set
### Release Models
The release is based on the Chinese model trained by `textgen`. The model has been released to HuggingFace models. Specifying the model name `textgen` will automatically download the model and can be used directly.

| Model                                                                                                     | Arch       | Introduce                                                                                                                                                                | Training                                                                                                                                     | Inference                                                                                                             | 
|:----------------------------------------------------------------------------------------------------------|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|
| [shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)             | T5         | ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹                                                                                                                                                         | [prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)                                  | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_prompt_demo.py)                       |
| [shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)                     | T5         | fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹                                                                                                                                                       | [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md) | [predict script](https://github.com/shibing624/textgen/blob/main/examples/t5/t5_couplet_demo.py)                      |
| [shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)                 | SongNet    | SongNeté¢„è®­ç»ƒæ¨¡å‹                                                                                                                                                             | -                                                                                                                                            | -                                                                                                                     |
| [shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)   | SongNet    | fine-tunedå®‹è¯åçš„æ¨¡å‹                                                                                                                                                         | [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                              | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_songci_demo.py)             |
| [shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet) | SongNet    | fine-tunedå¯¹è”åçš„æ¨¡å‹                                                                                                                                                         | [training script](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)                                 | [predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet/songnet_couplet_demo.py)            |
| [shibing624/chatglm-6b-csc-zh-lora](https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora)             | ChatGLM-6B | åœ¨27ä¸‡ä¸­æ–‡æ‹¼å†™çº é”™æ•°æ®[shibing624/CSC](https://huggingface.co/datasets/shibing624/CSC)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œçº é”™æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡                                                        | [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_csc_demo.py)                             | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/csc_demo.py)                        |
| [shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)         | ChatGLM-6B | åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆChatGLM-6Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡                           | [training script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py)                       | [predict script](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_hfdataset_demo.py) |
| [shibing624/llama-13b-belle-zh-lora](https://huggingface.co/shibing624/llama-13b-belle-zh-lora)           | LLaMA-13B  | åœ¨100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)ä¸Šå¾®è°ƒäº†ä¸€ç‰ˆLlama-13Bï¼Œé—®ç­”æ•ˆæœæœ‰æå‡ï¼Œå‘å¸ƒå¾®è°ƒåçš„LoRAæƒé‡                            | [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_hfdataset_demo.py)     |
| [shibing624/chinese-alpaca-plus-7b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-7b-hf)       | LLaMA-7B   | [ä¸­æ–‡LLaMA-Plus, Alpaca-Plus 7Bç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v3.0)ï¼Œåœ¨LLaMA-7Bä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨å¹¶ç»§ç»­é¢„è®­ç»ƒ120Gæ–‡æœ¬ï¼ˆé€šç”¨é¢†åŸŸï¼‰ï¼Œåœ¨4MæŒ‡ä»¤æ•°æ®é›†ä¸Šå¾®è°ƒåå¾—åˆ°çš„ä¸­æ–‡Alpaca-plusæ¨¡å‹     | [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)     |
| [shibing624/chinese-alpaca-plus-13b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf)     | LLaMA-13B  | [ä¸­æ–‡LLaMA-Plus, Alpaca-Plus 13Bç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v3.1)ï¼Œåœ¨LLaMA-13Bä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨å¹¶ç»§ç»­é¢„è®­ç»ƒ120Gæ–‡æœ¬ï¼ˆé€šç”¨é¢†åŸŸï¼‰ï¼Œåœ¨4.3MæŒ‡ä»¤æ•°æ®é›†ä¸Šå¾®è°ƒåå¾—åˆ°çš„ä¸­æ–‡Alpaca-plusæ¨¡å‹ | [training script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)                           | [predict script](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)     |

### Evaluation

| Model                                                                                                                                       | Arch       | Introduce                                                                                                                                                                                                                                                                                     | Score    |
|:--------------------------------------------------------------------------------------------------------------------------------------------|:-----------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------|
| [LLaMA-7B-Chinese-Alpaca](https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b)                                                         | LLaMA-7B   | å¤ç”¨[ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/examples/README.md)çš„è¯„ä¼°caseå’Œå¾—åˆ†                                                                                                                                                                          | 4.92     |
| [LLaMA-13B-Chinese-Alpaca](https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b)                                                       | LLaMA-13B  | å¤ç”¨[ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/examples/README.md)çš„è¯„ä¼°caseå’Œå¾—åˆ†                                                                                                                                                                          | 7.05     |
| [ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b)                                                                                       | ChatGLM-6B | åŸºäºåŸç”Ÿ`THUDM/chatglm-6b`è¯„ä¼°æµ‹è¯•é›†å¾—åˆ†                                                                                                                                                                                                                                                                 | 7.16     |
| [ChatGLM-6B-v1.1](https://huggingface.co/THUDM/chatglm-6b)                                                                                  | ChatGLM-6B | åŸºäºåŸç”Ÿ`THUDM/chatglm-6b`v1.1è‹±æ–‡ä¼˜åŒ–ç‰ˆæ¨¡å‹è¯„ä¼°æµ‹è¯•é›†å¾—åˆ†                                                                                                                                                                                                                                                      | **7.18** |
| [shibing624/chatglm-6b-belle-zh-lora](https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora)                                           | ChatGLM-6B | åŸºäº`THUDM/chatglm-6b`åŠ è½½`shibing624/chatglm-6b-belle-zh-lora`LoRAæ¨¡å‹åè¯„ä¼°æµ‹è¯•é›†å¾—åˆ†                                                                                                                                                                                                                     | 7.03     |
| [facat/alpaca-lora-cn-13b](https://huggingface.co/facat/alpaca-lora-cn-13b)	                                                                | LLaMA-13B  | åŸºäº`decapoda-research/llama-13b-hf`åŠ è½½`facat/alpaca-lora-cn-13b`LoRAæ¨¡å‹åè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                                                               | 4.13     |  
| [Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco](https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco) | LLaMA-13B  | åŸºäº`decapoda-research/llama-13b-hf`åŠ è½½`Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco`LoRAæ¨¡å‹åè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                               | 3.98     |
| [shibing624/chinese-alpaca-plus-7b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-7b-hf)                                         | LLaMA-7B   | ä½¿ç”¨[ymcui/Chinese-LLaMA-Alpaca åˆå¹¶æ¨¡å‹æ–¹æ³•](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus)åˆå¹¶HFæƒé‡åï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ† | 6.93     |
| [shibing624/chinese-alpaca-plus-13b-hf](https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf)                                       | LLaMA-13B  | ä½¿ç”¨[ymcui/Chinese-LLaMA-Alpaca åˆå¹¶æ¨¡å‹æ–¹æ³•](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2#%E5%A4%9Alora%E6%9D%83%E9%87%8D%E5%90%88%E5%B9%B6%E9%80%82%E7%94%A8%E4%BA%8Echinese-alpaca-plus)åˆå¹¶HFæƒé‡åï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ† | 7.07     |
| [TheBloke/vicuna-13B-1.1-HF](https://huggingface.co/TheBloke/vicuna-13B-1.1-HF)                                                             | LLaMA-13B  | ä½¿ç”¨åŸç”Ÿvicuna-13B-1.1åˆå¹¶åçš„æ¨¡å‹ï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                                                                                                           | 5.13     |
| [IDEA-CCNL/Ziya-LLaMA-13B-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1)                                                           | LLaMA-13B  | ä½¿ç”¨å§œå­ç‰™é€šç”¨å¤§æ¨¡å‹V1ï¼Œè¯„ä¼°æµ‹è¯•é›†å¹¶æ ‡æ³¨å¾—åˆ†                                                                                                                                                                                                                                                                       | 6.63     |

Evaluation conclusion:

- Evaluation case, see the online document for details: Chinese LLM-benchmark multi-task evaluation set (Tencent document) https://docs.qq.com/sheet/DUUpsREtWbFBsUVJE?tab=r7io7g Thanks to Han Junming, [Yang Jiaming](https:// github.com/yangjiam) and other students' annotations
- Evaluation task types include: knowledge quiz, open-ended question and answer, numerical calculation, poetry, music, sports, entertainment, article writing, text translation, code programming, ethics, refusal, multi-round question and answer, Score score is the top 100 ( 10-point scale) average score, manually scored, the higher the better
- The number of evaluations is small, the types of tasks are not comprehensive enough, the size relationship between the scores has some reference value, and the absolute value of the score is not much reference value
- Evaluation script: [tests/test_benchmark.py](https://github.com/shibing624/textgen/blob/main/tests/test_benchmark.py), using fp16 prediction, no int quantization processing, running the script can reproduce the evaluation However, the generated results are random and are affected by factors such as decoding hyperparameters and random seeds. The evaluation is not absolutely rigorous, and the test results are for reference only
- Conclusion: The performance of the Chinese derivative models of ChatGLM-6B and LLaMA-13B (including alpaca-plus, vicuna, ziya) belongs to the first echelon, and the performance of the original LLaMA-7B is slightly worse overall
- LLaMA-13B-Chinese-Alpaca is an instruction fine-tuning model that expands the Chinese vocabulary on the original LLaMA and incorporates about 20G of general Chinese corpus, which shows that LLaMA has an excellent base and strong language transfer capabilities
- ChatGLM, a native Chinese pre-training model, understands Chinese semantics better, and scores high in Chinese knowledge questions and answers and open questions and answers
- High scores in numerical calculation, Chinese-English translation, and code programming of LLaMA series models
- The Chinese-LLaMA model after Chinese pre-training and SFT fine-tuning has improved scores in Chinese poetry, entertainment, and ethics compared with the original LLaMA model

## ğŸš€ Demo

HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate

![](docs/hf.png)

run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:

```shell
python examples/gradio_demo.py
```

model trained by [examples/t5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/t5/T5_Finetune_Chinese_Couplet.ipynb)

## ğŸ’¾ Install

```shell
pip install -U textgen
```

or

install develop version:
```shell
pip install torch # conda install pytorch
git clone https://github.com/shibing624/textgen.git
cd textgen
python setup.py install
```

## â–¶ï¸ Usage

### ChatGLM-6B Model

#### Fine-tuned model using ChatGLM-6B

example: [examples/chatglm/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/predict_demo.py)

```python
from textgen import ChatGlmModel

model = ChatGlmModel("chatglm", "THUDM/chatglm-6b", peft_name="shibing624/chatglm-6b-csc-zh-lora")
r = model.predict(["å¯¹ä¸‹é¢ä¸­æ–‡æ‹¼å†™çº é”™ï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚\nç­”ï¼š"])
print(r)  # ['å°‘å…ˆé˜Ÿå‘˜åº”è¯¥ä¸ºè€äººè®©åº§ã€‚\né”™è¯¯å­—ï¼šå› ï¼Œå']
```

PS: Due to the use of the peft library under development, the loading of the LoRA model may fail due to the version update. It is recommended to use the following training method to train the LoRA model by yourself.

#### Train the ChatGLM-6B fine-tuning model

1. Support custom training data sets and training parameters, the data set format reference [examples/data/zh_csc_test.tsv](https://github.com/shibing624/textgen/blob/main/examples/data/zh_csc_test.tsv) Or [shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
2. Support some parameter fine-tuning methods such as AdaLoRA, LoRA, P_Tuning, Prefix_Tuning, etc., and also support full parameter fine-tuning
3. Support multi-card training and mixed precision training

example: [examples/chatglm/training_chatglm_demo.py](https://github.com/shibing624/textgen/blob/main/examples/chatglm/training_chatglm_demo.py)

Training with Single GPUï¼š
```shell
cd examples/chatglm
CUDA_VISIBLE_DEVICES=0 python training_chatglm_demo.py --do_train --do_predict --num_epochs 1 --output_dir outputs_chatglm
```

Training with Multi GPUï¼š
```shell
cd examples/chatglm
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 training_chatglm_demo.py --do_train --do_predict --num_epochs 20
```

#### Continue training based on fine-tuning (LoRA) model
If you need to continue training based on the Lora model, you can use the following script to merge the model into a new base model, and then fine-tune the training.

Execute the following command:
```shell
python -m textgen/chatglm/merge_peft_adapter.py \
     --base_model_name_or_path path_to_original_base_model_dir \
     --peft_model_path path_to_peft_model_dir \
     --output_dir path_to_output_dir
```
Parameter Description:
```
--base_model_name_or_path: directory to store base model weights and configuration files in HF format
--peft_model_path: directory for storing fine-tuning model weights and configuration files in PEFT format
--output_dir: Specify the directory to save the weight of the full model, the default is ./merged
```

### LLaMA model

#### Fine-tuned model using LLaMA
example: [examples/llama/predict_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/predict_demo.py)

<details>
<summary>show code example and result</summary>

```python
import sys

sys.path.append('../..')
from textgen import GptModel


def generate_prompt(instruction):
  return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:{instruction}\n\n### Response:"""


model = GptModel("llama", "decapoda-research/llama-7b-hf", peft_name="ziqingyang/chinese-alpaca-lora-7b")
predict_sentence = generate_prompt("é—®ï¼šç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\nç­”ï¼š")
r = model.predict([predict_sentence])
print(r)  # ['åœ°çƒæ˜¯å”¯ä¸€ä¸€é¢—æ‹¥æœ‰ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚']
```

</details>

#### Train the LLaMA fine-tuning model
1. Support custom training data sets and training parameters, the data set format reference [examples/data/zh_csc_test.tsv](https://github.com/shibing624/textgen/blob/main/examples/data/zh_csc_test.tsv) Or [shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
2. Support some parameter fine-tuning methods such as AdaLoRA, LoRA, P_Tuning, Prefix_Tuning, etc., and also support full parameter fine-tuning
3. Support multi-card training, support mixed precision training, use the same method as above (ChatGLM multi-GPU training)

example: [examples/llama/training_llama_demo.py](https://github.com/shibing624/textgen/blob/main/examples/llama/training_llama_demo.py)


#### Continue training based on fine-tuning (LoRA) model
If you need to continue training based on the Lora model, you can use the following script to merge the model into a new base model, and then fine-tune the training.

Single LoRA weight merging (for Chinese-LLaMA, Chinese-LLaMA-Plus, Chinese-Alpaca)

Execute the following command:
```shell
python -m textgen/llama/merge_peft_adapter.py \
    --base_model_name_or_path path_to_original_base_model_dir \
    --peft_model_path path_to_chinese_llama_or_alpaca_lora \
    --output_type [pth|huggingface]
    --output_dir path_to_output_dir 
```
Parameter Description:
```
--base_model_name_or_path: directory to store base model weights and configuration files in HF format
--peft_model_path: The directory where the Chinese LLaMA/Alpaca LoRA file is decompressed. You can also use the Lora model name on HF. For example, `ziqingyang/chinese-alpaca-lora-7b` will automatically download the corresponding model
--output_type: Specifies the output format, which can be pth or huggingface. If not specified, the default is huggingface
--output_dir: Specify the directory to save the weight of the full model, the default is ./merged
--offload_dir (optional): For low memory users need to specify an offload cache path
```

#### Training Domain Model

| Notebook | Description | |
|:----------|:------------|------:|
| [training_medical_model.ipynb](https://github.com/shibing624/textgen/blob/main/examples/llama/training_medical_model.ipynb) | Training medical large model|[![Open In Colab](https://colab .research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shibing624/textgen/blob/main/examples/llama/training_medical_model.ipynb) |

Note: In order to comprehensively introduce the process of training large medical models, a new repo has been created for the 4-stage training method (Pretraining, Supervised Finetuning, Reward Modeling and Reinforcement Learning): [shibing624/MedicalGPT](https://github.com/ shibing624/MedicalGPT), please move to this repo to view the training method.

### BLOOM model

#### Train the BLOOM fine-tuning model

example: [examples/bloom/training_bloom_demo.py](https://github.com/shibing624/textgen/blob/main/examples/bloom/training_bloom_demo.py)

### ConvSeq2Seq æ¨¡å‹

Train and predict the ConvSeq2Seq model:

example: [examples/seq2sesq/training_convseq2seq_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_convseq2seq_model_demo.py)

<details>
<summary>show code example and result</summary>

```python
import argparse
from loguru import logger
import sys

sys.path.append('../..')
from textgen.seq2seq.conv_seq2seq_model import ConvSeq2SeqModel


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')
    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')
    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')
    parser.add_argument('--output_dir', default='./outputs/convseq2seq_zh/', type=str, help='Model output directory')
    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')
    parser.add_argument('--num_epochs', default=200, type=int, help='Number of training epochs')
    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')
    args = parser.parse_args()
    logger.info(args)

    if args.do_train:
        logger.info('Loading data...')
        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,
                                 model_dir=args.output_dir, max_length=args.max_seq_length)
        model.train_model(args.train_file)
        print(model.eval_model(args.train_file))

    if args.do_predict:
        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,
                                 model_dir=args.output_dir, max_length=args.max_seq_length)
        sentences = ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
        print("inputs:", sentences)
        print('outputs:', model.predict(sentences))


if __name__ == '__main__':
    main()
```

output:

```bash
inputs: ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„å»ºæ€ç»´çš„æœºå™¨ã€‚', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•è¿è„‘ä¸Šå·¥ä½œï¼', 'æˆ‘ä¸èƒ½é”™çƒ­æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„äººå·¥æ™ºèƒ½"200å¹´ã€‚']
```

</details>

### BART Model
Train and predict the BART model:

example: [examples/seq2sesq/training_bartseq2seq_zh_demo.py](https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_bartseq2seq_zh_demo.py)

output:

```shell
inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Š', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦å—ï¼Ÿ']
```

### T5 Model

example: [examples/t5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/t5/training_zh_t5_model_demo.py)

<details>
<summary>show code example and result</summary>

```python
import argparse
from loguru import logger
import pandas as pd
import sys

sys.path.append('../..')
from textgen.t5 import T5Model


def load_data(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip('\n')
            terms = line.split('\t')
            if len(terms) == 2:
                data.append(['QA', terms[0], terms[1]])
            else:
                logger.warning(f'line error: {line}')
    return data


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')
    parser.add_argument('--model_type', default='t5', type=str, help='Transformers model type')
    parser.add_argument('--model_name', default='Langboat/mengzi-t5-base', type=str, help='Transformers model or path')
    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')
    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')
    parser.add_argument('--output_dir', default='./outputs/mengzi_t5_zh/', type=str, help='Model output directory')
    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')
    parser.add_argument('--num_epochs', default=10, type=int, help='Number of training epochs')
    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')
    args = parser.parse_args()
    logger.info(args)

    if args.do_train:
        logger.info('Loading data...')
        # train_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.
        #   - `prefix`: A string indicating the task to perform. (E.g. `"question"`, `"stsb"`)
        #   - `input_text`: The input text. `prefix` is prepended to form the full input. (<prefix>: <input_text>)
        #   - `target_text`: The target sequence
        train_data = load_data(args.train_file)
        logger.debug('train_data: {}'.format(train_data[:10]))
        train_df = pd.DataFrame(train_data, columns=["prefix", "input_text", "target_text"])

        eval_data = load_data(args.train_file)[:10]
        eval_df = pd.DataFrame(eval_data, columns=["prefix", "input_text", "target_text"])

        model_args = {
            "reprocess_input_data": True,
            "overwrite_output_dir": True,
            "max_seq_length": args.max_seq_length,
            "train_batch_size": args.batch_size,
            "num_train_epochs": args.num_epochs,
            "save_eval_checkpoints": False,
            "save_model_every_epoch": False,
            "evaluate_generated_text": True,
            "evaluate_during_training": True,
            "evaluate_during_training_verbose": True,
            "use_multiprocessing": True,
            "save_best_model": True,
            "output_dir": args.output_dir,
            "use_early_stopping": True,
        }
        # model_type: t5  model_name: Langboat/mengzi-t5-base
        model = T5Model(args.model_type, args.model_name, args=model_args)

        def count_matches(labels, preds):
            logger.debug(f"labels: {labels[:10]}")
            logger.debug(f"preds: {preds[:10]}")
            match = sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])
            logger.debug(f"match: {match}")
            return match

        model.train_model(train_df, eval_data=eval_df, matches=count_matches)
        print(model.eval_model(eval_df, matches=count_matches))

    if args.do_predict:
        model = T5Model(args.model_type, args.output_dir)
        sentences = ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
        print("inputs:", sentences)
        print("outputs:", model.predict(sentences))


if __name__ == '__main__':
    main()
```

output:

```shell
inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
```

</details>

### GPT2 Model

#### Chinese GPT2 - Article Generation

Use the Chinese dataset (paragraph format, `\n` interval) to train the GPT2 model, which can be used for poetry generation, article generation and other tasks.

example: [examples/gpt2/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_zh_gpt2_demo.py)

#### Chinese GPT2 - couplet generation

Use the Chinese couplet dataset (tsv format, `\t` interval), customize the dataset to read the Dataset, and train the GPT2 model, which can be used for couplet generation, dialogue generation and other tasks.

example: [examples/gpt2/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/gpt2/training_couplet_gpt2_demo.py)

GPT2 vs T5ï¼š

1. Both are improved from Transformer, T5 has both encoder and decoder, GPT2 only has decoder
2. The advantage of the T5 model is to process a given input and output tasks corresponding to the output, such as translation, dialogue, question and answer, etc.
3. The advantage of the GPT2 model is free creation, such as writing a short article
4. The couplet generation effect of T5 is better than that of GPT2, and the poetry generation effect of GPT2 is better than that of T5

- [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
- [å¤è¯—ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%8F%A4%E8%AF%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)

### SongNet æ¨¡å‹

Format-controlled text generation model, see paper [SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022),
It is suitable for tasks such as poetry, couplets, and lyrics generation that require strong rhythmic formats.

example: [examples/songnet/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/songnet/training_zh_songnet_demo.py)

### Keyword Text Augmentation(EDA/UDA)

example: [examples/text_augmentation/text_augmentation_demo.py](examples/text_augmentation/text_augmentation_demo.py)

<details>
<summary>show code example and result</summary>

```python
import sys

sys.path.append('..')
from textgen.augment import TextAugment

if __name__ == '__main__':
    docs = ['ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹',
            'æ™šä¸Šè‚šå­å¥½éš¾å—',
            'ä½ ä¼šæ­¦åŠŸå—ï¼Œæˆ‘ä¸ä¼š',
            'ç»„è£…æ ‡é¢˜è´¨é‡å—é™äºå¹¿å‘Šä¸»è‡ªæç‰©æ–™çš„ç‰‡æ®µè´¨é‡ï¼Œä¸”è¡¨è¾¾ä¸°å¯Œåº¦æœ‰é™',
            ]
    m = TextAugment(sentence_list=docs)
    a = docs[0]
    print(a)

    b = m.augment(a, aug_ops='random-0.2')
    print('random-0.2:', b)

    b = m.augment(a, aug_ops='insert-0.2')
    print('insert-0.2:', b)

    b = m.augment(a, aug_ops='delete-0.2')
    print('delete-0.2:', b)

    b = m.augment(a, aug_ops='tfidf-0.2')
    print('tfidf-0.2:', b)

    b = m.augment(a, aug_ops='mix-0.2')
    print('mix-0.2:', b)
```

output:

```bash
ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹
random-0.2: ('ä¸»è¦é™ªé™ªæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ä¸»è¦è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå—é™äºå†…å®¹', [('ç ”ç©¶', 'é™ªé™ª', 2, 4), ('ã€', 'ä¸»è¦', 13, 15), ('ç›¸å…³', 'å—é™äº', 27, 30)])
insert-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨æœºå™¨å­¦ä¹ å­¦ä¹ ã€æ·±åº¦æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æœºå™¨', 'æœºå™¨æœºå™¨', 4, 8), ('å­¦ä¹ ', 'å­¦ä¹ å­¦ä¹ ', 8, 12), ('æ·±åº¦', 'æ·±åº¦æ·±åº¦', 13, 17)])
delete-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('æ™ºèƒ½', '', 20, 20)])
tfidf-0.2: ('ä¸€æ˜¯ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½äº¤è°ˆç³»ç»Ÿå¯†åˆ‡ç›¸å…³å†…å®¹', [('ä¸»è¦', 'ä¸€æ˜¯', 0, 2), ('è§†è§‰', 'å¬è§‰', 17, 19), ('å¯¹è¯', 'äº¤è°ˆ', 22, 24), ('ç›¸å…³', 'å¯†åˆ‡ç›¸å…³', 26, 30)])
mix-0.2: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ã€è®¡ç®—æœºå¬è§‰ã€æ™ºèƒ½å¯¹è¯è½¯ä»¶ç³»ç»Ÿç›¸å…³å†…å®¹', [('å­¦ä¹ ', 'å­¦', 11, 12), ('è§†è§‰', 'å¬è§‰', 16, 18), ('ç³»ç»Ÿ', 'è½¯ä»¶ç³»ç»Ÿ', 23, 27)])
```
</details>

### TGLS model (unsupervised similar text generation model)

Unsupervised generation of Chinese e-commerce reviews: Extract short sentences expressing opinions from users from **e-commerce reviews** and combine them to generate simulated reviews.

example: [examples/unsup_generation/unsup_generation_demo.py](examples/unsup_generation/unsup_generation_demo.py)

<details>
<summary>show code example and result</summary>

```python
import os
import sys

sys.path.append('..')
from textgen.unsup_generation import TglsModel, load_list

pwd_path = os.path.abspath(os.path.dirname(__file__))

samples = load_list(os.path.join(pwd_path, './data/ecommerce_comments.txt'))
docs_text = [
    ["æŒºå¥½çš„ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä¹Ÿå¾ˆå®æƒ ï¼Œä¸çŸ¥æ•ˆæœå¦‚ä½•",
     "äº§å“æ²¡å¾—è¯´ï¼Œä¹°äº†ä»¥åå°±é™ä»·ï¼Œå¿ƒæƒ…ä¸ç¾ä¸½ã€‚",
     "åˆšæ”¶åˆ°ï¼ŒåŒ…è£…å¾ˆå®Œæ•´ï¼Œä¸é”™",
     "å‘è´§é€Ÿåº¦å¾ˆå¿«ï¼Œç‰©æµä¹Ÿä¸é”™ï¼ŒåŒä¸€æ—¶é—´ä¹°çš„ä¸¤ä¸ªä¸œä¸œï¼Œä¸€ä¸ªå…ˆåˆ°ä¸€ä¸ªè¿˜åœ¨è·¯ä¸Šã€‚è¿™ä¸ªæ°´æ°´å¾ˆå–œæ¬¢ï¼Œä¸è¿‡ç›–å­çœŸçš„å¼€äº†ã€‚ç›–ä¸ç‰¢äº†ç°åœ¨ã€‚",
     "åŒ…è£…çš„å¾ˆå¥½ï¼Œæ˜¯æ­£å“",
     "è¢«ç§è‰å…°è”»ç²‰æ°´ä¸‰ç™¾å…ƒä¸€å¤§ç“¶å›¤è´§ï¼Œå¸Œæœ›æ˜¯æ­£å“å¥½ç”¨ï¼Œæ”¶åˆ°çš„æ—¶å€™ç”¨ä¿é²œè†œåŒ…è£¹å¾—ä¸¥ä¸¥å®å®ï¼Œåªæ•¢ä¹°è€ƒæ‹‰è‡ªè¥çš„æŠ¤è‚¤å“",
     ],
    ['å¾ˆæ¸©å’Œï¼Œæ¸…æ´—çš„ä¹Ÿå¾ˆå¹²å‡€ï¼Œä¸æ²¹è…»ï¼Œå¾ˆä¸é”™ï¼Œä¼šè€ƒè™‘å›è´­ï¼Œç¬¬ä¸€æ¬¡è€ƒæ‹‰ä¹°æŠ¤è‚¤å“ï¼Œæ»¡æ„',
     'è¿™æ¬¾å¸å¦†æ²¹æˆ‘ä¼šæ— é™å›è´­çš„ã€‚å³ä½¿æˆ‘æ˜¯æ²¹ç—˜çš®ï¼Œä¹Ÿä¸ä¼šé—·ç—˜ï¼ŒåŒæ—¶åœ¨è„¸éƒ¨æŒ‰æ‘©æ—¶ï¼Œè¿˜èƒ½è§£å†³ç™½å¤´çš„è„‚è‚ªç²’çš„é—®é¢˜ã€‚ç”¨æ¸…æ°´æ´—å®Œè„¸åï¼Œéå¸¸çš„æ¸…çˆ½ã€‚',
     'è‡ªä»ç”¨äº†fanclä¹‹åå°±ä¸ç”¨å…¶ä»–å¸å¦†äº†ï¼Œå¸çš„èˆ’æœåˆå¹²å‡€',
     'ä¹°è´µäº†ï¼Œå¤§æ¶¦å‘æ‰å–79ã€‚9ã€‚',
     ],
    samples
]
m = TglsModel(docs_text)
r = m.generate(samples[:500])
print('size:', len(r))
for review in r:
    print('\t' + review)
```

output:

[ç¾è¿ªæƒ å°” N.M.Fé’ˆå‰‚æ°´åº“ä¿æ¹¿é¢è†œ](https://goods.kaola.com/product/2227311.html)æœ‰å¦‚ä¸‹çš„20å¥è¯„è®ºï¼Œå…¶ä¸­æœ‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œ10å¥æ˜¯ç”Ÿæˆçš„è¯„è®ºï¼Œèƒ½çœ‹å‡ºæ¥ä¹ˆ?ğŸ˜‚

```
è¿˜ä¸é”™è¿˜ä¸é”™è¿˜ä¸é”™è¿˜ä¸é”™ã€‚
ä¸œè¥¿åˆ°äº†ï¼Œä¸çŸ¥é“å¥½ä¸å¥½ç”¨ã€‚è¯•ç”¨è¿‡åå†æ¥è¯„ä»·ã€‚åˆ°æ—¶çœ‹ç½‘è¯„éƒ½è¿˜å¯ä»¥ã€‚
å“ºä¹³æœŸå”¯ä¸€ä½¿ç”¨çš„æŠ¤è‚¤å“ï¼Œæ¯å¤©éƒ½æ˜¯ç´ é¢œï¼Œè„¸é¢å…¨é é¢è†œåŠç€ğŸ˜„è¡¥æ°´ğŸ’¦ä¸ç²˜è…»ä¸€å¦‚æ—¢å¾€çš„æ”¯æŒï¼Œå–œæ¬¢ğŸ’•
ææ´»åŠ¨æ—¶ä¹°çš„é¢è†œï¼Œä¸çŸ¥é“è¿™ä¸ªé¢è†œæ˜¯çœŸæ˜¯å‡æ•·åœ¨è„¸ä¸Šé¢è†œçº¸éƒ½æœ‰å°æ°´æ³¡é¼“èµ·æ¥ã€‚
å¾ˆä¸é”™ï¼Œéå¸¸è¡¥æ°´ï¼Œç”¨è¿‡çš„éƒ½çŸ¥é“ï¼Œæ€§ä»·æ¯”ä¹‹ç‹ï¼Œå¥½ç”¨åˆä¸è´µï¼Œæ­£å“ï¼Œç”¨ç€æ”¾å¿ƒï¼Œç‰©æµä¹Ÿå¾ˆå¿«ã€‚
é¢è†œéå¸¸å¥½ç”¨å“¦ã€‚é¢è†œè–„è–„çš„ã€‚å¥½åƒæ˜¯èš•ä¸é¢è†œå•Šã€‚ç²¾åå¾ˆå¤šå‘¢ã€‚æ•·åœ¨è„¸ä¸Šå¾ˆèˆ’æœã€‚æ„Ÿè§‰æŒºä¿æ¹¿çš„ï¼Œå‘³é“ä¹ŸæŒºå¥½é—»çš„ã€‚å°±æ˜¯é‡Œé¢åªæœ‰å•çº¯çš„é¢è†œç›´æ¥æ•·è„¸ä¸Šæœ‰ç‚¹ä¸å¥½å¼„ï¼Œå“ˆå“ˆå“ˆ
è¿˜å¯ä»¥ä¿æ¹¿æ•ˆæœä¸é”™æ°´æ¶¦æ¶¦çš„æ¯å¤©è´´ä¸€ç‰‡è„¸ä¹Ÿä¸å¹²äº†ç”¨å®Œäº†åœ¨ä¹°ç‚¹ï¼Œä¸é”™è¿˜ä¼šç»§ç»­å›è´­çš„ã€‚
å¿«é€’å¾ˆå¿«ï¼Œä¸œè¥¿å¾ˆèµï¼æƒ³è¦å¾—ç‚¹è€ƒæ‹‰è±†ä¸å®¹æ˜“ï¼Œè¿˜è¦ä¸‰åä¸ªå­—ã€‚æ—¶é—´å®è´µï¼ŒåºŸè¯ä¸è¯´ï¼ç”¨è¿‡äº†å°±çŸ¥é“äº†
æŒºå¥½ç”¨çš„ï¼Œæœ‹å‹æ¨èæ¥çš„
æŒºå¥½ç”¨çš„ï¼Œæ·¡æ·¡çš„ï¼Œè™½ç„¶ä¸æ˜¯å¾ˆæµ“ç²¾åçš„æ„Ÿè§‰ï¼Œä½†æ˜¯æ•ˆæœä¹Ÿè›®å¥½çš„ã€‚åˆ’ç®—
ä¸å¾—ä¸è¯´ç¾è¿ªæƒ å°”çš„é¢è†œæ˜¯æˆ‘ç”¨è¿‡çš„æœ€å¥½çš„é¢è†œä¹‹ä¸€ğŸ˜è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œæ²¡æƒ³åˆ°è¿™ä¹ˆä¾¿å®œçš„ä»·æ ¼ç«ŸçœŸçš„èƒ½ä¹°åˆ°çœŸå“ã€‚
ä¿æ¹¿æ•ˆæœæŒºå¥½çš„ï¼Œé¢è†œå¾ˆå¥½ç”¨ã€‚
æœŸå¾…å¥½çš„äº§å“ã€‚
ä¸€æ‰“å¼€åŒ…è£…é‡Œé¢çš„ç²¾ååˆšåˆšå¥½ï¼Œç”¨äº†è¡¥æ°´è¡¥æ°´æ•ˆæœä¸é”™ï¼Œç‰©æµéå¸¸å¿«ã€‚
çš®è‚¤å¾ˆå…‰æ»‘ğŸ˜‡æ¯”ä¸Šå»é€Ÿåº¦å¿«ä¸‰å¤©å°±åˆ°äº†ã€‚
å‰ä¸¤å¤©çš®è‚¤å¹²ç‡¥è¿ç»­æ•·äº†ä¸¤ä¸ªæ™šä¸Šæ„Ÿè§‰è¿˜ä¸é”™ğŸ˜‚è¡¥æ°´æ•ˆæœæ˜æ˜¾ï¼å¯æƒ³è€ŒçŸ¥ç²¾åæ¶²åˆå¤šå……è¶³ğŸ˜æ•·ä¸Šä»¥åå‡‰å‡‰çš„å¾ˆèˆ’æœã€‚
è¡¥æ°´æ•ˆæœä¸€èˆ¬å§ï½ä½†æ˜¯æˆ‘ç”¨çš„éŸ©å›½èƒŒå›æ¥çš„é¢è†œçº¸ä¸ç®—è–„ï¼Œå¸Œæœ›å¥½ç”¨ä¼šå›è´­çš„ï¼Œæ•·ä¸Šè„¸æ„Ÿè§‰æ¯”è¾ƒæ¸…çˆ½ï½ä»·æ ¼è¿˜ä¸ä¾¿å®œã€‚
å¸Œæœ›å¥½ç”¨ï¼Œé¢è†œç”¨è¿‡äº†å¾ˆå¥½ç”¨ï¼Œçš®è‚¤æ°´å«©å…‰æ»‘ç™½çš™ï¼Œè¡¥æ°´ä¸é”™ï¼Œä»·æ ¼ä¹Ÿåˆé€‚ã€‚
å°±æ˜¯ç²¾åæ¶²å¤ªå°‘äº†ï¼Œä¿æ¹¿æ•ˆæœä¸é”™ã€‚
é¢è†œçš„è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œä¿æ¹¿æ•ˆæœç¡®å®å¾ˆèµï¼Œè¿™ä¸ªé¢è†œç›¸å¯¹äºèƒ¶åŸè›‹ç™½å’Œç¾ç™½çš„é‚£ä¸¤æ¬¾çš„é¢è†œçº¸è¦åšä¸€äº›ï¼Œçœ‹ç€ä»·æ ¼åˆé€‚ã€‚
```

The first 10 sentences are real user reviews, and the last 10 sentences are generated.

</details>

## ğŸ“š Dataset 

1. Belle dataset of 500,000 Chinese ChatGPT commands: [BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
2. Belle dataset of 1 million Chinese ChatGPT commands: [BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
3. Alpaca dataset of 50,000 English ChatGPT commands: [50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
4. Alpaca dataset of 20,000 Chinese ChatGPT commands: [shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
5. Guanaco dataset with 690,000 Chinese instructions (500,000 Belle + 190,000 Guanaco): [Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
6. 2.4 million Chinese medical data sets (including pre-training data and instruction fine-tuning data sets): [shibing624/medical](https://huggingface.co/datasets/shibing624/medical)

## âœ… Todo

1. [ ] Added multi-round dialogue data fine-tuning method
2. [x] add reward model finetuning
3. [x] add rl finetuning
4. [x] add medical reward dataset
5. [x] add llama in4 training
6. [ ] add all training and predict demo in colab

## â˜ï¸ Contact

- Issue (suggestion)
   : [![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
- Email me: xuming: xuming624@qq.com
- WeChat Me: Add me* WeChat ID: xuming624, Remarks: Name-Company Name-NLP* Enter the NLP exchange group.

<img src="docs/wechat.jpeg" width="200" />

## ğŸ˜‡ Citation

If you use textgen in your research, please cite it in the following format:

```latex
@misc{textgen,
  title={textgen: Text Generation Tool},
  author={Ming Xu},
  year={2021},
  howpublished={\url{https://github.com/shibing624/textgen}},
}
```

## ğŸ¤— License

The authorization agreement is [The Apache License 2.0](/LICENSE), which can be used for commercial purposes free of charge. Please attach textgen's link and license agreement in the product description.

## ğŸ˜ Contribute

The project code is still rough. If you have improved the code, you are welcome to submit it back to this project. Before submitting, please pay attention to the following two points:

- Add corresponding unit tests in `tests`
- Use `python -m pytest` to run all unit tests to ensure that all unit tests are passed

Then you can submit a PR.

## ğŸ’• Acknowledgements 

- [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
- [minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn)
- [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)
- [asyml/texar](https://github.com/asyml/texar)
- [yangjianxin1/GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat)
- [williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch)
- [RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)
- [Tiiiger/bert_score](https://github.com/Tiiiger/bert_score)
- [ThilinaRajapakse/simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)
- [1YCxZ/Fake-review-generation](https://github.com/1YCxZ/Fake-review-generation)
- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)

Thanks for their great work!
