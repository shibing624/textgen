# -*- coding: utf-8 -*-
"""
@author:XuMing(xuming624@qq.com)
@description: æ— ç›‘ç£æŠ½å–ç”¨æˆ·è§‚ç‚¹
å‚è€ƒ: https://github.com/rainarch/SentiBridge

jiebaåˆ†è¯çš„æ•ˆæœè¿˜ä¸è¶³ä»¥æ”¯æŒç”µå•†è¯„è®ºï¼Œä¾‹å¦‚"ç—˜ç—˜è‚Œ"ã€"ç‚’é¸¡æ£’"ã€"tå­—åŒº"ç­‰è¯æ˜¯jiebaæ— æ³•å¤„ç†çš„ã€‚
æ–°å¢æ–°è¯å‘ç°åŠŸèƒ½(PMI+å·¦å³ç†µ)çš„æ–¹æ³•æ¥æ‰¾å‡ºæ–°è¯ï¼Œå‚è€ƒï¼šhttps://www.matrix67.com/blog/archives/5044
"""
import random
from loguru import logger
import os
import jieba
import jieba.posseg
import math
import re

pwd_path = os.path.abspath(os.path.dirname(__file__))
jieba.setLogLevel(log_level="ERROR")
WINDOW_SIZE = 5
PUNCTUATION_MARK = ['x']  # æ ‡ç‚¹
PUNCTUATION = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ï½']
NOUN_MARK = ['n', 'ng', 'nr', 'nrfg', 'nrt', 'ns', 'nt', 'nz']  # åè¯
VERB_MARK = ['v', 'vd', 'vg', 'vi', 'vn', 'vq']  # åŠ¨è¯
ADJECTIVE_MARK = ['a', 'ad', 'an', 'ag']  # å½¢å®¹è¯
ADVERB_MARK = ['d', 'df', 'dg']  # å‰¯è¯
ENG_MARK = ['eng']
EMOJI = ['ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜†', 'ğŸ˜‰', 'ğŸ˜Š',
         'ğŸ˜‹', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜˜', 'ğŸ˜—', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜‡',
         'ğŸ˜', 'ğŸ˜']
YANWENZI = ['ãƒ½(âœ¿ï¾Ÿâ–½ï¾Ÿ)ãƒ', 'Ï†(â‰§Ï‰â‰¦*)â™ª', 'â•°(*Â°â–½Â°*)â•¯', 'o(ï¿£â–½ï¿£)ï½„', 'o( =â€¢Ï‰â€¢= )m']
ILLEGAL_WORD = ['è€ƒæ‹‰', 'ç½‘æ˜“', 'æ·˜å®', 'äº¬ä¸œ', 'æ‹¼å¤šå¤š', 'ä¸è¿‡', 'å› ä¸º', 'è€Œä¸”', 'ä½†æ˜¯', 'ä½†', 'æ‰€ä»¥', 'å› æ­¤', 'å¦‚æœ']  # è¿‡æ»¤è¯

RESERVED_MARK = NOUN_MARK + VERB_MARK + ADJECTIVE_MARK + ADVERB_MARK + ENG_MARK  # ç”¨äºå‘ç°æ–°è¯
ASPECT_MARK = NOUN_MARK + VERB_MARK

PUNCTUATION_MAP = {".": "ã€‚", ",": "ï¼Œ", "!": "ï¼", "?": "ï¼Ÿ", "~": "ï½"}
keep_p = ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï½', 'ã€']


def convert(content):
    """è½¬åŒ–æ ‡ç‚¹ç¬¦å·ä¸ºä¸­æ–‡ç¬¦å·"""
    nc = []
    for c in content:
        if c in PUNCTUATION_MAP:
            nc.append(PUNCTUATION_MAP[c])
            continue
        nc.append(c)
    return "".join(nc)


def clean(line):
    """æ¸…æ´—æ— æ„ä¹‰å­—ç¬¦"""
    if line == "":
        return
    line = convert(line)
    c_content = []
    for char in line:
        if re.search("[\u4e00-\u9fa5]", char):
            c_content.append(char)
        elif re.search("[a-zA-Z0-9]", char):
            c_content.append(char)
        elif char in keep_p:
            c_content.append(char)
        elif char == ' ':  # å¾ˆå¤šç”¨æˆ·å–œæ¬¢ç”¨ç©ºæ ¼æ›¿ä»£æ ‡ç‚¹
            c_content.append('ï¼Œ')
        else:
            c_content.append('')
    nc_content = []
    c = 0
    for char in c_content:
        if char in keep_p:
            c += 1
        else:
            c = 0
        if c < 2:
            nc_content.append(char)
    result = ''.join(nc_content)
    result = result.strip()
    result = result.lower()  # æ‰€æœ‰è‹±æ–‡è½¬æˆå°å†™å­—æ¯
    return result


def clean_review(text):
    """
    å¯¹åŸå§‹è¯„è®ºè¿›è¡Œæ¸…ç†ï¼Œåˆ å»éæ³•å­—ç¬¦ï¼Œç»Ÿä¸€æ ‡ç‚¹ï¼Œåˆ å»æ— ç”¨è¯„è®º
    """
    review_set = []
    for line in text:
        line = line.lstrip()
        line = line.rstrip()
        line = clean(line)
        if len(line) < 7:  # è¿‡äºçŸ­çš„è¯„è®ºéœ€è¦åˆ é™¤
            continue
        if line and line not in ['è¯¥ç”¨æˆ·æ²¡æœ‰å¡«å†™è¯„è®ºã€‚', 'ç”¨æˆ·æ™’å•ã€‚']:
            review_set.append(line)

    return review_set


def text2review(seg_pos_text):
    """
    ç»è¿‡åˆ†è¯çš„æ–‡æ¡£ï¼Œå¾—åˆ°åŸå§‹ç”¨æˆ·çš„æ¯æ¡è¯„è®º
    """
    review_list = []  # ä¿å­˜å…¨éƒ¨çš„æŒ‰ç…§æŒ‡å®šæ ‡ç‚¹åˆ‡åˆ†çš„å¥å­
    all_word = set()  # å…¨éƒ¨å•è¯
    for seg_pos in seg_pos_text:
        cur_review = []
        for term in seg_pos:
            word, flag = term.split('/')
            cur_review.append(word)
            if flag in RESERVED_MARK:
                all_word.add(word)
        review_list.append(cur_review)

    return review_list, all_word


def find_word_phrase(all_word, seg_list):
    """
    æ ¹æ®ç‚¹äº’ä¿¡æ¯ä»¥åŠä¿¡æ¯ç†µå‘ç°è¯ç»„ï¼Œä¸»è¦ç›®çš„æ˜¯æå‡åˆ†è¯æ•ˆæœ
    """
    res = []
    word_count = {k: 0 for k in all_word}  # è®°å½•å…¨éƒ¨è¯å‡ºç°çš„æ¬¡æ•°

    all_word_count = 0
    all_bi_gram_count = 0
    for sentence in seg_list:
        all_word_count += len(sentence)
        all_bi_gram_count += len(sentence) - 1
        for idx, word in enumerate(sentence):
            if word in word_count:
                word_count[word] += 1

    bi_gram_count = {}
    bi_gram_lcount = {}
    bi_gram_rcount = {}
    for sentence in seg_list:
        for idx, _ in enumerate(sentence):
            left_word = sentence[idx - 1] if idx != 0 else ''
            right_word = sentence[idx + 2] if idx < len(sentence) - 2 else ''

            first = sentence[idx]
            second = sentence[idx + 1] if idx + 1 < len(sentence) else ''
            if first in word_count and second in word_count:
                if (first, second) in bi_gram_count:
                    bi_gram_count[(first, second)] += 1
                else:
                    bi_gram_count[(first, second)] = 1
                    bi_gram_lcount[(first, second)] = {}
                    bi_gram_rcount[(first, second)] = {}

                if left_word in bi_gram_lcount[(first, second)]:
                    bi_gram_lcount[(first, second)][left_word] += 1
                elif left_word != '':
                    bi_gram_lcount[(first, second)][left_word] = 1

                if right_word in bi_gram_rcount[(first, second)]:
                    bi_gram_rcount[(first, second)][right_word] += 1
                elif right_word != '':
                    bi_gram_rcount[(first, second)][right_word] = 1

    bi_gram_count = dict(filter(lambda x: x[1] >= 5, bi_gram_count.items()))

    bi_gram_le = {}  # å…¨éƒ¨bi_gramçš„å·¦ç†µ
    bi_gram_re = {}  # å…¨éƒ¨bi_gramçš„å³ç†µ
    for phrase in bi_gram_count:
        le = 0
        for l_word in bi_gram_lcount[phrase]:
            p_aw_w = bi_gram_lcount[phrase][l_word] / bi_gram_count[phrase]  # P(aW | W)
            le += p_aw_w * math.log2(p_aw_w)
        le = -le
        bi_gram_le[phrase] = le

    for phrase in bi_gram_count:
        re = 0
        for r_word in bi_gram_rcount[phrase]:
            p_wa_w = bi_gram_rcount[phrase][r_word] / bi_gram_count[phrase]  # P(Wa | W)
            re += p_wa_w * math.log2(p_wa_w)
        re = -re
        bi_gram_re[phrase] = re

    PMI = {}
    for phrase in bi_gram_count:
        p_first = word_count[phrase[0]] / all_word_count
        p_second = word_count[phrase[1]] / all_word_count
        p_bi_gram = bi_gram_count[phrase] / all_bi_gram_count
        PMI[phrase] = math.log2(p_bi_gram / (p_first * p_second))

    phrase_score = []
    for phrase in PMI:
        le = bi_gram_le[phrase]
        re = bi_gram_re[phrase]
        score = PMI[phrase] + le + re
        phrase_score.append((phrase, score))

    phrase_score = sorted(phrase_score, key=lambda x: x[1], reverse=True)

    for item in phrase_score:
        res.append('{}:{}'.format(''.join(item[0]), item[1]))

    return res


def load_list(path):
    return [l for l in open(path, 'r', encoding='utf-8').read().split()]


def caculate_word_idf(docs, stopwords):
    """
    è®¡ç®—æ‰€æœ‰æ–‡æ¡£ä¸­çš„æ¯ä¸ªè¯çš„idf
    docs: list(list(str)), æ•°æ®é›†
    stop_word: list, åœç”¨è¯list

    return: æ‰€æœ‰è¯çš„idfå€¼
    """
    word_IDF = {}  # word-IDF è®°å½•æ¯ä¸ªwordåœ¨ä¸åŒçš„docå‡ºç°è¿‡çš„æ¬¡æ•°,ç„¶åè®¡ç®—IDF
    num_doc = len(docs)  # å•†å“æ•°é‡
    seg_pos_text = []
    for doc in docs:
        cur_doc_word_set = set()  # è®°å½•å½“å‰æ–‡æ¡£ä¸­å‡ºç°çš„ä¸åŒçš„è¯
        for line in doc:
            line = line.strip()
            seg_pos_list = get_seg_pos(line, type='word')
            seg_pos_text.append(seg_pos_list)
            word_list = [term.split('/')[0] for term in seg_pos_list]
            for w in word_list:
                # å¦‚æœè¿™ä¸ªè¯åœ¨åœç”¨è¯è¡¨ä¸­å°±ä¸æ·»åŠ 
                if w in stopwords:
                    continue
                cur_doc_word_set.add(w)
        for w in cur_doc_word_set:
            if w in word_IDF:
                word_IDF[w] += 1
            else:
                word_IDF[w] = 1
    for w in word_IDF:
        word_IDF[w] = math.log10(num_doc / word_IDF[w])
    return word_IDF, seg_pos_text


def get_seg_pos(line, type='word'):
    """
    è·å–æ–‡æ¡£çš„åˆ†è¯ä»¥åŠè¯æ€§æ ‡æ³¨ç»“æœï¼Œåˆ†è¯çš„æ–¹å¼å¯ä»¥ä¸ºæŒ‰è¯åˆ‡åˆ†æˆ–è€…æŒ‰å­—åˆ‡åˆ†
    """
    if type == 'word':
        line_cut = jieba.posseg.cut(line.strip())
        wordlist = []
        for term in line_cut:
            wordlist.append('%s/%s' % (term.word, term.flag))
        res = wordlist
    else:
        res = list(line.strip())
    return res


def text2seg_pos(seg_pos_text, pattern='[ã€‚ï¼ï¼Ÿ]'):
    """
    ç»è¿‡åˆ†è¯çš„æ–‡æ¡£ï¼ŒåŸå§‹ä¸€æ¡ç”¨æˆ·è¯„è®ºé€šè¿‡æŒ‡å®šçš„æ ‡ç‚¹ç¬¦å·åˆ†æˆå¤šä¸ªå¥å­
    """
    seg_list = []  # ä¿å­˜å…¨éƒ¨æŒ‰æ ‡ç‚¹åˆ‡åˆ†çš„seg
    pos_list = []  # ä¿å­˜å…¨éƒ¨æŒ‰æ ‡ç‚¹åˆ‡åˆ†çš„pos
    seg_review_list = []  # ç”¨æˆ·å®Œæ•´çš„ä¸€æ¡è¯„è®º
    for seg_pos in seg_pos_text:
        seg_sub_list = []
        pos_sub_list = []
        cur_review = []
        for term in seg_pos:
            word, flag = term.split('/')
            cur_review.append(word)
            if word in pattern:
                seg_sub_list.append(word)
                pos_sub_list.append(flag)
                seg_list.append(list(seg_sub_list))
                pos_list.append(list(pos_sub_list))
                seg_sub_list = []
                pos_sub_list = []
            else:
                seg_sub_list.append(word)
                pos_sub_list.append(flag)
        seg_review_list.append(list(cur_review))

    return seg_list, pos_list, seg_review_list


def get_candidate_aspect(seg_list, pos_list, adj_word, stop_word, word_idf):
    """
    è¾“å…¥çš„æ•°æ®ä¸ºç”¨é€—å·éš”å¼€çš„çŸ­å¥ï¼Œ
    åˆ©ç”¨å¼€çª—å£çš„æ–¹å¼ï¼Œæ ¹æ®æƒ…æ„Ÿè¯å…¸æŠ½åè¯å¾—åˆ°å€™é€‰çš„aspect
    """
    aspect_dict = {}
    for i, sentence in enumerate(seg_list):
        for j, word in enumerate(sentence):
            if word in adj_word and pos_list[i][j] in ADJECTIVE_MARK:  # å½“å‰çš„è¯å±äºæƒ…æ„Ÿè¯ä¸”è¯æ€§ä¸ºå½¢å®¹è¯
                startpoint = j - WINDOW_SIZE
                startpoint = startpoint if startpoint >= 0 else 0
                for k in range(startpoint, j):
                    if pos_list[i][k] in ASPECT_MARK:
                        if seg_list[i][k] in aspect_dict:
                            aspect_dict[seg_list[i][k]] += 1
                        else:
                            aspect_dict[seg_list[i][k]] = 1

    candidates = aspect_dict.items()
    candidates = list(filter(lambda x: len(x[0]) > 1, candidates))  # ç»è¿‡è¯ç»„å‘ç°ä¹‹åï¼Œåˆ å»ä¸€ä¸ªå­—çš„è¯
    candidates = [item[0] for item in candidates if item[0] not in stop_word]  # åˆ å»åœç”¨è¯
    candidates = [item if (item in word_idf and word_idf[item] != 0) else item for item in candidates]  # åˆ å»IDFå€¼ä¸º0çš„è¯
    logger.debug(f"Extract {len(candidates)} aspect candidates, top10: {candidates[:10]}")
    return candidates


class NSDict:
    """
    ç”¨æ¥æ„å»ºå€™é€‰é›†ï¼ˆaspectï¼Œopinionï¼Œpatternï¼‰
    """

    def __init__(self, seg_list, pos_list, raw_aspect_list):
        self.seg_list = seg_list
        self.pos_list = pos_list
        self.raw_aspect_list = raw_aspect_list
        self.ns_dict = {}
        self.aspect_do_not_use = []
        self.opinion_do_not_use = ["æœ€", "ä¸", "å¾ˆ"]
        self.pattern_do_not_use = ["çš„-", "å’Œ-", "å’Œ+", "è€Œ+", "è€Œ-", "åˆ+", "åˆ-", "è€Œä¸”+", "è€Œä¸”-"]

    def _seg2nsd(self, aspect_for_filter):
        for x, clue in enumerate(self.seg_list):
            N_list = []
            S_list = []
            word_list = clue
            for y, word in enumerate(clue):
                if word in aspect_for_filter:
                    N_list.append(y)
                elif self.pos_list[x][y] in ADJECTIVE_MARK:
                    S_list.append(y)
            if N_list and S_list:
                self._make_nsdict(word_list, N_list, S_list)

    def _make_nsdict(self, word_list, N_list, S_list):
        for n in N_list:
            for s in S_list:
                if (1 < n - s < WINDOW_SIZE + 1) or (1 < s - n < WINDOW_SIZE + 1):  # çª—å£å¤§å°æ˜¯5
                    if word_list[n] not in self.ns_dict:
                        self.ns_dict[word_list[n]] = {}
                    if word_list[s] not in self.ns_dict[word_list[n]]:
                        self.ns_dict[word_list[n]][word_list[s]] = {}
                    if n > s:
                        patt = ' '.join(word_list[s + 1: n]) + '+'
                    else:
                        patt = ' '.join(word_list[n + 1: s]) + '-'
                    if patt not in self.ns_dict[word_list[n]][word_list[s]]:
                        self.ns_dict[word_list[n]][word_list[s]][patt] = 0.
                    self.ns_dict[word_list[n]][word_list[s]][patt] += 1.

    def _noise_del(self):
        for aspect in self.aspect_do_not_use:
            self._noise(aspect, self.ns_dict)
        for n in self.ns_dict:
            for opinion in self.opinion_do_not_use:
                self._noise(opinion, self.ns_dict[n])
            for s in self.ns_dict[n]:
                for pattern in self.pattern_do_not_use:
                    self._noise(pattern, self.ns_dict[n][s])

    def _noise(self, str, dict):
        if str in dict:
            del dict[str]

    def build_nsdict(self):
        """Stage 1ï¼šextract pair and pattern"""
        self._seg2nsd(self.raw_aspect_list)
        self._noise_del()
        return self.ns_dict


class PairPattSort:
    """
    Pair-Patt-Count structure
    """

    def __init__(self, ns_dict):
        self._get_map(ns_dict)

    def _get_map(self, ns_dict):
        """
        get map: [pair-patt], [patt-pair], [pair](score), [patt](score)

        :param ns_dict: Entity.str { Emotion.str { Pattern.str { Count.int (It's a three-level hash structure)
        :return:
        """
        pair_list = []
        patt_dict = {}
        patt_pair_map = {}
        pair_patt_map = {}

        aspects = list(ns_dict.keys())
        aspects.sort()

        for n in aspects:
            for s in ns_dict[n]:
                n_s = "{}\t{}".format(n, s)  # è¿™é‡Œå­˜çš„pairæ˜¯å­—ç¬¦ä¸²ï¼Œä¸­é—´ç”¨\téš”å¼€
                pair_list.append(n_s)
                pair_patt_map[n_s] = {}
                for patt in ns_dict[n][s]:
                    if patt not in patt_dict:
                        patt_dict[patt] = 1.0
                    pair_patt_map[n_s][patt] = ns_dict[n][s][patt]
                    if patt in patt_pair_map:
                        patt_pair_map[patt][n_s] = ns_dict[n][s][patt]
                    else:
                        patt_pair_map[patt] = {}
                        patt_pair_map[patt][n_s] = ns_dict[n][s][patt]
        self.patt_pair_map = patt_pair_map
        self.pair_patt_map = pair_patt_map
        self.pair_len = len(pair_list)
        self.patt_len = len(patt_dict)
        self.pair_score = dict([(word, 1.) for i, word in enumerate(pair_list)])
        self.patt_score = patt_dict

    def _norm(self, score_dict, score_len):
        """
        æ­£åˆ™åŒ–ï¼Œå’Œä¸ºscore_len
        """
        sum_score = 0.
        for s in score_dict:
            sum_score += score_dict[s]
        for s in score_dict:
            score_dict[s] = score_dict[s] / sum_score * score_len
        return score_dict

    def _patt_pair(self):
        for pair in self.pair_patt_map:  # <- å¾ªç¯éå†æ¯ä¸ªpair
            value = 0.
            for patt in self.pair_patt_map[pair]:  # <- æ¯ä¸ªpairä¸­çš„patternå‡ºç°çš„ä¸ªæ•° * è¿™ä¸ªpatternçš„scoreï¼Œç„¶åæ±‚å’Œå¾—åˆ°è¿™ä¸ªpairçš„åˆ†æ•°
                value += self.pair_patt_map[pair][patt] * self.patt_score[patt]
            self.pair_score[pair] = value

    def _pair_patt(self):
        for patt in self.patt_pair_map:  # <- éå†æ¯ä¸ªpattern
            value = 0.
            for pair in self.patt_pair_map[patt]:  # <- æ¯ä¸ªè¢«patternä¿®é¥°çš„pairå‡ºç°çš„ä¸ªæ•° * è¿™ä¸ªpairçš„scoreï¼Œç„¶åæ±‚å’Œå¾—åˆ°è¿™ä¸ªpattern1çš„
                value += self.patt_pair_map[patt][pair] * self.pair_score[pair]
            self.patt_score[patt] = value

    def _patt_correct(self):
        self.patt_score['çš„-'] = 0.0

    def _iterative(self):
        """
        A complete iteration
        [pair] = [patt-pair] * [patt]
        [patt] = [pair-patt] * [pair]
        :return:
        """
        self._patt_pair()
        self.pair_score = self._norm(self.pair_score, self.pair_len)
        self._pair_patt()
        self.patt_score = self._norm(self.patt_score, self.patt_len)

    def sort_pair(self):
        """Stage 2ï¼špair sort"""
        for i in range(100):
            self._iterative()
        pair_score = sorted(self.pair_score.items(), key=lambda d: d[1], reverse=True)
        return pair_score


def get_aspect_express(seg_review_list, pair_useful):
    """
    æŠ½å–åŸå§‹è¯„è®ºä¸­çš„aspectä½œä¸ºè¾“å…¥ï¼Œå®Œæ•´çš„è¯„è®ºä½œä¸ºè¾“å‡º
    """

    def check_sentence(sentence):
        """
        åˆ¤æ–­çŸ­å¥æ˜¯å¦åˆæ³•
        """
        _s = ''.join(sentence)
        legal = True
        if len(_s) > 30:
            legal = False
        return legal

    raw_aspect_express = {k: [] for k in pair_useful}  # ç”¨æˆ·å…³äºæŸä¸ªè§‚ç‚¹çš„ä¸€æ®µåŸå§‹è¡¨è¾¾
    raw_aspect_express_count = {k: 0 for k in pair_useful}  # è®°å½•æŸä¸ªè§‚ç‚¹è¡¨è¾¾å‡ºç°çš„æ¬¡æ•°
    for review in seg_review_list:  # æ¯ä¸ªsentenceå°±æ˜¯ä¸€å¥å®Œæ•´çš„review
        if review[-1] not in PUNCTUATION:
            review.append('ã€‚')

        # å¯¹äºå•ä¸ªreviewè¿›è¡Œåˆ‡åˆ†
        cur_review = []
        pre_end = 0
        for i, _ in enumerate(review):
            if review[i] in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ï½']:
                cur_review.append(review[pre_end:i + 1])
                pre_end = i + 1
            elif i == len(review) - 1:
                cur_review.append(review[pre_end:])

        for sentence in cur_review:  # sentence æ˜¯ä¸¤ä¸ªæ ‡ç‚¹ä¹‹é—´çš„çŸ­å¥
            if sentence[-1] not in PUNCTUATION:
                sentence.append('ã€‚')
            find_opinion_flag = False
            for idx, word in enumerate(sentence):
                if find_opinion_flag:  # å¦‚æœåœ¨å½“å‰çš„çŸ­å¥ä¸­å·²ç»æ‰¾åˆ°äº†ä¸€ç»„è§‚ç‚¹è¡¨è¾¾å°±ç»“æŸå¯¹è¿™ä¸ªçŸ­å¥çš„æœç´¢
                    break
                if word in pair_useful:  # å½“å‰çš„wordå±äºaspect
                    # å‘å‰å¼€çª—å£
                    startpoint = idx - WINDOW_SIZE if idx - WINDOW_SIZE > 0 else 0
                    for i in range(startpoint, idx):  # å¯»æ‰¾opinion word
                        cur_word = sentence[i]
                        if cur_word in pair_useful[word] and sentence[i + 1] == "çš„":  # eg. è¶…èµçš„ä¸€æ¬¾é¢è†œ
                            if check_sentence(sentence):
                                raw_aspect_express[word].append(sentence)
                                raw_aspect_express_count[word] += 1
                                find_opinion_flag = True  # åªè¦æ‰¾åˆ°ä¸€ä¸ªopinion wordå°±ç®—å‘½ä¸­ä¸€ä¸ªçŸ­å¥äº†

                    # å‘åå¼€çª—å£
                    endpoint = idx + WINDOW_SIZE if idx + WINDOW_SIZE < len(sentence) else len(sentence)
                    for i in range(idx + 1, endpoint):
                        cur_word = sentence[i]
                        if cur_word in pair_useful[word]:
                            if check_sentence(sentence):
                                raw_aspect_express[word].append(sentence)
                                raw_aspect_express_count[word] += 1
                                find_opinion_flag = True  # åªè¦æ‰¾åˆ°ä¸€ä¸ªopinion wordå°±ç®—å‘½ä¸­ä¸€ä¸ªçŸ­å¥äº†
    # ç­›é€‰å¾—åˆ°ä¿ç•™çš„aspect
    aspect_express = {}
    for aspect in raw_aspect_express:
        if raw_aspect_express_count[aspect] < 5:
            continue
        aspect_express[aspect] = raw_aspect_express[aspect]

    return aspect_express


def merge_aspect_express(aspect_express, pair_useful):
    """
    å¯¹ç›¸ä¼¼çš„è§‚ç‚¹è¡¨è¾¾è¿›è¡Œåˆå¹¶, åŒæ—¶è¾“å‡ºæœ€ç»ˆçš„aspect_opinion_pair
    """
    aspects = list(aspect_express.keys())
    aspects.sort()  # æ’æˆå­—å…¸åº
    merged_aspects = [[aspects[0]]] if aspects else [[]]
    merged_express = {}
    opinion_set = []

    def check_is_same(word1, word2):
        """
        åˆ¤æ–­ä¸¤ä¸ªè¯å½“ä¸­æ˜¯å¦å­˜åœ¨ç›¸åŒçš„å­—
        """
        for i in word1:
            if i in word2:
                return True
        return False

    for i in range(1, len(aspects)):
        if check_is_same(merged_aspects[-1][-1], aspects[i]):
            merged_aspects[-1].append(aspects[i])
        else:
            merged_aspects.append([aspects[i]])
    for a_list in merged_aspects:
        # æ”¶é›†å…¨éƒ¨çš„å½¢å®¹è¯
        for i in a_list:
            opinion_set += pair_useful[i]

        _l = ','.join(a_list)
        merged_express[_l] = []
        for i in a_list:
            merged_express[_l] += aspect_express[i]
    opinion_set = set(opinion_set)
    return merged_express, opinion_set


def build_dataset_express(seg_review_list, pair_useful):
    """
    æŠ½å–åŸå§‹è¯„è®ºä¸­çš„aspectä½œä¸ºè¾“å…¥ï¼Œå®Œæ•´çš„è¯„è®ºä½œä¸ºè¾“å‡º
    """
    train_data = []  # è®°å½•è®­ç»ƒæ•°æ®
    for review in seg_review_list:  # æ¯ä¸ªsentenceå°±æ˜¯ä¸€å¥å®Œæ•´çš„review

        source = []  # è®­ç»ƒçš„src
        if review[-1] not in PUNCTUATION:
            review.append('ã€‚')
        target = review  # è®­ç»ƒçš„tgt

        # å¯¹äºå•ä¸ªreviewè¿›è¡Œåˆ‡åˆ†
        cur_review = []
        pre_end = 0
        for i, _ in enumerate(review):
            if review[i] in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ï½']:
                cur_review.append(review[pre_end:i + 1])
                pre_end = i + 1
            elif i == len(review) - 1:
                cur_review.append(review[pre_end:])

        for sentence in cur_review:  # sentence æ˜¯ä¸¤ä¸ªæ ‡ç‚¹ä¹‹é—´çš„çŸ­
            if sentence[-1] not in PUNCTUATION:
                sentence.append('ã€‚')
            find_opinion_flag = False
            for idx, word in enumerate(sentence):
                if find_opinion_flag:  # å¦‚æœåœ¨å½“å‰çš„çŸ­å¥ä¸­å·²ç»æ‰¾åˆ°äº†ä¸€ç»„è§‚ç‚¹è¡¨è¾¾å°±ç»“æŸå¯¹è¿™ä¸ªçŸ­å¥çš„æœç´¢
                    break
                if word in pair_useful:  # å½“å‰çš„wordå±äºaspect
                    source.append(word)
                    find_opinion_flag = True  # åªè¦æ‰¾åˆ°ä¸€ä¸ªopinion wordå°±ç®—å‘½ä¸­ä¸€ä¸ªçŸ­å¥äº†
        train_data.append((list(source), target))
    max_source_length = 0

    # ç­›é€‰è®­ç»ƒæ•°æ®
    def check_review(item):
        """
        åˆ¤æ–­å½“å‰reviewæ˜¯å¦åˆæ³•
        """
        source = item[0]
        tgt = item[1]
        legal = True
        _s = ''.join(tgt)
        if len(source) == 0 or len(source) > 5:  # ä¸å«æœ‰è§‚ç‚¹è¡¨è¾¾æˆ–è€…è§‚ç‚¹è¯å¤ªå¤š
            legal = False
        unique_source = set(source)
        if len(unique_source) != len(source):
            legal = False
        if len(_s) > 60:
            legal = False
        return legal

    legal_train_data = []
    for item in train_data:
        if check_review(item):
            max_source_length = max(max_source_length, len(item[0]))
            legal_train_data.append(item)

    logger.debug(f'max source length: {max_source_length}')
    return legal_train_data


def generate_reviews(aspect_express, num_steps=1000):
    """
    æ ¹æ®å€™é€‰é›†åˆç”Ÿæˆå‡è¯„è®º
    """
    res = []
    all_aspect = list(aspect_express.keys())
    logger.debug(f'Aspect: {all_aspect}')

    # æ ¹æ®ä¸åŒaspectå‡ºç°çš„æ¦‚ç‡åˆ†é…ä¸åŒæƒé‡
    aspect_length_dict = {}
    for a in aspect_express:
        aspect_length_dict[a] = len(aspect_express[a])
    weight_aspect_list = []
    for aspect in aspect_length_dict:
        weight_aspect_list += [aspect] * aspect_length_dict[aspect]
    if not weight_aspect_list:
        return res
    for _ in range(num_steps):
        num_aspect = random.choice([1, 2, 3, 4, 5, 6])
        review = []
        used_aspect = []
        for _ in range(num_aspect):
            a = random.choice(weight_aspect_list)
            if a in used_aspect and len(all_aspect) > 1:
                a = random.choice(weight_aspect_list)
            used_aspect.append(a)
            a_s = random.choice(aspect_express[a])
            a_s = a_s[:-1] + ['#']  # ä¸¢æ‰æ ‡ç‚¹ï¼Œæ¢ä½#ä½œä¸ºåˆ‡åˆ†ç‚¹
            review += a_s
        res.append(review)
    return res


def fake_review_filter(reviews, opinion_set, is_uniq=True):
    """
    ç­›å»è¯„è®ºä¸­ä¸åƒäººå†™çš„å¥å­ï¼šå¦‚æœåŒä¸€ä¸ªå½¢å®¹è¯é‡å¤å‡ºç°ä¸¤æ¬¡å°±åˆ¤å®šä¸ºå‡è¯„è®ºï¼ŒåŒæ—¶ç­›å»é•¿åº¦è¶…è¿‡60çš„è¯„è®º
    """
    results = []
    for review in reviews:
        opinion_used = {k: 0 for k in opinion_set}
        flag = True
        for word in review:
            if word in ILLEGAL_WORD:
                flag = False
            if word in opinion_used:
                opinion_used[word] += 1
                if opinion_used[word] >= 2:
                    flag = False
                    break
        if flag:
            _s = ''.join(review)
            _s = _s.split('#')  # æœ€åä¸€ä¸ªæ˜¯ç©ºå­—ç¬¦
            review = ''
            pu = ['ï¼Œ'] * 100 + ['ï½'] * 20 + ['ï¼'] * 20 + EMOJI + YANWENZI
            random.shuffle(pu)
            for a_s in _s:
                if a_s:
                    review += a_s + random.choice(pu)
            if not review:
                logger.warning(f'error: {review}')
            review = review[:-1] + 'ã€‚'
            if is_uniq:
                if review not in results:
                    results.append(review)
            else:
                results.append(review)
    return results


if __name__ == '__main__':
    # ä½¿ç”¨äº†(PMI+å·¦å³ç†µ)çš„æ–¹æ³•æ¥æ‰¾å‡ºæ–°è¯
    default_stopwords_path = os.path.join(pwd_path, '../data/stopwords.txt')
    sample1 = load_list(os.path.join(pwd_path, '../../examples/data/ecommerce_comments_100.txt'))
    docs_text = [["æŒºå¥½çš„ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä¹Ÿå¾ˆå®æƒ ï¼Œä¸çŸ¥æ•ˆæœå¦‚ä½•",
                  "äº§å“æ²¡å¾—è¯´ï¼Œä¹°äº†ä»¥åå°±é™ä»·ï¼Œå¿ƒæƒ…ä¸ç¾ä¸½ã€‚",
                  "åˆšæ”¶åˆ°ï¼ŒåŒ…è£…å¾ˆå®Œæ•´ï¼Œä¸é”™",
                  "å‘è´§é€Ÿåº¦å¾ˆå¿«ï¼Œç‰©æµä¹Ÿä¸é”™ï¼ŒåŒä¸€æ—¶é—´ä¹°çš„ä¸¤ä¸ªä¸œä¸œï¼Œä¸€ä¸ªå…ˆåˆ°ä¸€ä¸ªè¿˜åœ¨è·¯ä¸Šã€‚è¿™ä¸ªæ°´æ°´å¾ˆå–œæ¬¢ï¼Œä¸è¿‡ç›–å­çœŸçš„å¼€äº†ã€‚ç›–ä¸ç‰¢äº†ç°åœ¨ã€‚",
                  "åŒ…è£…çš„å¾ˆå¥½ï¼Œæ˜¯æ­£å“",
                  "è¢«ç§è‰å…°è”»ç²‰æ°´ä¸‰ç™¾å…ƒä¸€å¤§ç“¶å›¤è´§ï¼Œå¸Œæœ›æ˜¯æ­£å“å¥½ç”¨ï¼Œæ”¶åˆ°çš„æ—¶å€™ç”¨ä¿é²œè†œåŒ…è£¹å¾—ä¸¥ä¸¥å®å®ï¼Œåªæ•¢ä¹°è€ƒæ‹‰è‡ªè¥çš„æŠ¤è‚¤å“",
                  ],
                 ['å¾ˆæ¸©å’Œï¼Œæ¸…æ´—çš„ä¹Ÿå¾ˆå¹²å‡€ï¼Œä¸æ²¹è…»ï¼Œå¾ˆä¸é”™ï¼Œä¼šè€ƒè™‘å›è´­ï¼Œç¬¬ä¸€æ¬¡è€ƒæ‹‰ä¹°æŠ¤è‚¤å“ï¼Œæ»¡æ„',
                  'è¿™æ¬¾å¸å¦†æ²¹æˆ‘ä¼šæ— é™å›è´­çš„ã€‚å³ä½¿æˆ‘æ˜¯æ²¹ç—˜çš®ï¼Œä¹Ÿä¸ä¼šé—·ç—˜ï¼ŒåŒæ—¶åœ¨è„¸éƒ¨æŒ‰æ‘©æ—¶ï¼Œè¿˜èƒ½è§£å†³ç™½å¤´çš„è„‚è‚ªç²’çš„é—®é¢˜ã€‚ç”¨æ¸…æ°´æ´—å®Œè„¸åï¼Œéå¸¸çš„æ¸…çˆ½ã€‚',
                  'è‡ªä»ç”¨äº†fanclä¹‹åå°±ä¸ç”¨å…¶ä»–å¸å¦†äº†ï¼Œå¸çš„èˆ’æœåˆå¹²å‡€',
                  'ä¹°è´µäº†ï¼Œå¤§æ¶¦å‘æ‰å–79ã€‚9ã€‚',
                  ],
                 sample1
                 ]
    print('docs_text len:', len(docs_text))
    # åŠ è½½åœç”¨è¯
    stopwords = set(load_list(default_stopwords_path))
    # è®¡ç®—é™¤å»åœç”¨è¯çš„æ¯ä¸ªè¯çš„idfå€¼
    word_idf, seg_pos_text = caculate_word_idf(docs_text, stopwords)

    review_list, all_word = text2review(seg_pos_text)

    phrase_list = find_word_phrase(all_word, review_list)
    print(phrase_list)
