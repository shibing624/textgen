[![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
[![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/textgen.svg)](https://github.com/shibing624/textgen/graphs/contributors)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)

# TextGen

ğŸŒˆ Implementation of Text Generation models.

**textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šUDAã€GPT2ã€Seq2Seqã€BARTã€T5ç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚

**Guide**

- [Question](#Question)
- [Solution](#Solution)
- [Feature](#Feature)
- [Install](#install)
- [Usage](#usage)
- [Contact](#Contact)
- [Reference](#reference)

# Question

æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ€ä¹ˆåšï¼Ÿ

# Solution

1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
3. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
4. ç”Ÿæˆæ¨¡å‹ï¼šSeq2Seqï¼ŒGPT2ï¼ŒT5ï¼ŒBARTç­‰

# Feature

- [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
- [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
- [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„è¾ƒé«˜è´¨é‡çš„ç›¸ä¼¼æ–‡æœ¬


# Demo

HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate

![](docs/hf.png)

run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:

```shell
python examples/gradio_demo.py
```

# Install

```
pip3 install torch # conda install pytorch
pip3 install -U textgen
```

or

```
pip3 install torch # conda install pytorch
git clone https://github.com/shibing624/textgen.git
cd textgen
python3 setup.py install
```

# Usage

## Text Augmentation(EDAã€UDAæ–‡æœ¬æ•°æ®å¢å¼º)

example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)

```python
import sys

sys.path.append('..')
from textgen.augment import TextAugment

if __name__ == '__main__':
    docs = ['ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹',
            'æ™šä¸Šè‚šå­å¥½éš¾å—',
            'ä½ ä¼šæ­¦åŠŸå—ï¼Œæˆ‘ä¸ä¼š',
            'ç»„è£…æ ‡é¢˜è´¨é‡å—é™äºå¹¿å‘Šä¸»è‡ªæç‰©æ–™çš„ç‰‡æ®µè´¨é‡ï¼Œä¸”è¡¨è¾¾ä¸°å¯Œåº¦æœ‰é™',
            'æ™šä¸Šä¸€ä¸ªäººå¥½å­¤å•ï¼Œæƒ³:æ‰¾é™„è¿‘çš„äººé™ªé™ªæˆ‘.',
            ]
    m = TextAugment(sentence_list=docs)
    a = docs[0]
    print(a)

    b = m.augment(a, aug_ops='random-0.1')
    print('random-0.1:', b)

    b = m.augment(a, aug_ops='insert-0.1')
    print('insert-0.1:', b)

    # tfidf
    b = m.augment(a, aug_ops='tfidf-0.2')
    print('tfidf-0.2:', b)

    b = m.augment(a, aug_ops='mix-0.1', similar_prob=0.1,
                  random_prob=0.4, delete_prob=0.3, insert_prob=0.2)
    print('mix-0.1:', b)

    b = m.augment(a, aug_ops='bt')
    print('bt:', b)
```

output:

```bash
ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹
random-0.1: ('ä¸»è¦çš„æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å—è®¡ç®—æœºè§†è§‰ã€å¥½å­¤å•å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('ç ”ç©¶', 'çš„', 2, 3), ('ã€', 'å—', 12, 13), ('æ™ºèƒ½', 'å¥½å­¤å•', 19, 22)])
insert-0.1: ('ä¸»è¦ç ”ç©¶æœºå™¨æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯å¯¹è¯ç³»ç»Ÿç³»ç»Ÿç›¸å…³å†…å®¹', [('æœºå™¨', 'æœºå™¨æœºå™¨', 4, 8), ('å¯¹è¯', 'å¯¹è¯å¯¹è¯', 24, 28), ('ç³»ç»Ÿ', 'ç³»ç»Ÿç³»ç»Ÿ', 28, 32)])
tfidf-0.2: ('ä¸»è¦åŸå› ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºç¡¬ä»¶è§†è§‰ã€æ™ºèƒ½åŒ–å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('ä¸»è¦', 'ä¸»è¦åŸå› ', 0, 4), ('è®¡ç®—æœº', 'è®¡ç®—æœºç¡¬ä»¶', 16, 21), ('æ™ºèƒ½', 'æ™ºèƒ½åŒ–', 24, 27)])
mix-0.1: ('ä¸»è¦å—é™äºæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€æ™ºèƒ½å¯¹è¯ç³»ç»Ÿç›¸å…³å†…å®¹', [('ç ”ç©¶', 'å—é™äº', 2, 5)])
bt: ('ä¸»è¦ç ”ç©¶æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰å’Œæ™ºèƒ½å¯¹è¯ç³»ç»Ÿ', [])
```

## Seq2Seq æ¨¡å‹

### ConvSeq2Seq
è®­ç»ƒå¹¶é¢„æµ‹ConvSeq2Seqæ¨¡å‹ï¼š

example: [examples/seq2sesq/training_convseq2seq_model_demo.py](examples/seq2seq/training_convseq2seq_model_demo.py)

```python
import argparse
from loguru import logger
import sys

sys.path.append('../..')
from textgen.seq2seq.conv_seq2seq_model import ConvSeq2SeqModel


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')
    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')
    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')
    parser.add_argument('--output_dir', default='./outputs/convseq2seq_zh/', type=str, help='Model output directory')
    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')
    parser.add_argument('--num_epochs', default=200, type=int, help='Number of training epochs')
    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')
    args = parser.parse_args()
    logger.info(args)

    if args.do_train:
        logger.info('Loading data...')
        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,
                                 model_dir=args.output_dir, max_length=args.max_seq_length)
        model.train_model(args.train_file)
        print(model.eval_model(args.train_file))

    if args.do_predict:
        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,
                                 model_dir=args.output_dir, max_length=args.max_seq_length)
        sentences = ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
        print("inputs:", sentences)
        print('outputs:', model.predict(sentences))


if __name__ == '__main__':
    main()
```

output:

```bash
inputs: ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„å»ºæ€ç»´çš„æœºå™¨ã€‚', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•è¿è„‘ä¸Šå·¥ä½œï¼', 'æˆ‘ä¸èƒ½é”™çƒ­æ˜¯ä¸€ä¸ªç–¯ç‹‚çš„äººå·¥æ™ºèƒ½"200å¹´ã€‚']
```

### BART
è®­ç»ƒå¹¶é¢„æµ‹BARTæ¨¡å‹ï¼š

example: [examples/seq2sesq/training_bartseq2seq_zh_demo.py](examples/seq2seq/training_bartseq2seq_zh_demo.py)


output:

```shell
inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
outputs: ['äººå·¥æ™ºèƒ½æ˜¯å·¥ç¨‹å’Œç§‘å­¦çš„åˆ†æ”¯,è‡´åŠ›äºæ„', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Š', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦å—ï¼Ÿ']
```


## T5 æ¨¡å‹

example: [examples/T5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/T5/training_zh_t5_model_demo.py)

```python
import argparse
from loguru import logger
import pandas as pd
import sys

sys.path.append('../..')
from textgen.t5 import T5Model


def load_data(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip('\n')
            terms = line.split('\t')
            if len(terms) == 2:
                data.append(['QA', terms[0], terms[1]])
            else:
                logger.warning(f'line error: {line}')
    return data


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')
    parser.add_argument('--model_type', default='t5', type=str, help='Transformers model type')
    parser.add_argument('--model_name', default='Langboat/mengzi-t5-base', type=str, help='Transformers model or path')
    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')
    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')
    parser.add_argument('--output_dir', default='./outputs/mengzi_t5_zh/', type=str, help='Model output directory')
    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')
    parser.add_argument('--num_epochs', default=10, type=int, help='Number of training epochs')
    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')
    args = parser.parse_args()
    logger.info(args)

    if args.do_train:
        logger.info('Loading data...')
        # train_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.
        #   - `prefix`: A string indicating the task to perform. (E.g. `"question"`, `"stsb"`)
        #   - `input_text`: The input text. `prefix` is prepended to form the full input. (<prefix>: <input_text>)
        #   - `target_text`: The target sequence
        train_data = load_data(args.train_file)
        logger.debug('train_data: {}'.format(train_data[:10]))
        train_df = pd.DataFrame(train_data, columns=["prefix", "input_text", "target_text"])

        eval_data = load_data(args.train_file)[:10]
        eval_df = pd.DataFrame(eval_data, columns=["prefix", "input_text", "target_text"])

        model_args = {
            "reprocess_input_data": True,
            "overwrite_output_dir": True,
            "max_seq_length": args.max_seq_length,
            "train_batch_size": args.batch_size,
            "num_train_epochs": args.num_epochs,
            "save_eval_checkpoints": False,
            "save_model_every_epoch": False,
            "evaluate_generated_text": True,
            "evaluate_during_training": True,
            "evaluate_during_training_verbose": True,
            "use_multiprocessing": True,
            "save_best_model": True,
            "output_dir": args.output_dir,
            "use_early_stopping": True,
        }
        # model_type: t5  model_name: Langboat/mengzi-t5-base
        model = T5Model(args.model_type, args.model_name, args=model_args)

        def count_matches(labels, preds):
            logger.debug(f"labels: {labels[:10]}")
            logger.debug(f"preds: {preds[:10]}")
            match = sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])
            logger.debug(f"match: {match}")
            return match

        model.train_model(train_df, eval_data=eval_df, matches=count_matches)
        print(model.eval_model(eval_df, matches=count_matches))

    if args.do_predict:
        model = T5Model(args.model_type, args.output_dir)
        sentences = ["ä»€ä¹ˆæ˜¯ai", "ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº", "ä½ çŸ¥é“çƒ­åŠ›å­¦å—"]
        print("inputs:", sentences)
        print("outputs:", model.predict(sentences))


if __name__ == '__main__':
    main()
```

output:
```shell
inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
```

## GPT2 æ¨¡å‹

### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ

ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚

example: [examples/language_generation/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_gpt2_demo.py)

### ä¸­æ–‡GPT2 - å¯¹è”ç”Ÿæˆ

ä½¿ç”¨ä¸­æ–‡å¯¹è”æ•°æ®é›†ï¼ˆtsvæ ¼å¼ï¼Œ`\t`é—´éš”ï¼‰ï¼Œè‡ªå®šä¹‰æ•°æ®é›†è¯»å–Datasetï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºå¯¹è”ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚

example: [examples/language_generation/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_couplet_gpt2_demo.py)

#### GPT2 vs T5ï¼š
1. éƒ½æ˜¯ä»Transformeræ”¹è¿›æ¥çš„ï¼ŒT5åŒæ—¶æœ‰ç¼–ç å™¨å’Œè§£ç å™¨ï¼ŒGPT2åªæœ‰è§£ç å™¨
2. T5çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯å¤„ç†ç»™å®šè¾“å…¥ï¼Œäº§å‡ºå¯¹åº”è¾“å‡ºçš„ä»»åŠ¡ï¼Œå¦‚ç¿»è¯‘ã€å¯¹è¯ã€é—®ç­”ç­‰
3. GPT2çš„æ¨¡å‹ä¼˜åŠ¿æ˜¯è‡ªç”±åˆ›ä½œï¼Œå¦‚å†™ä¸€ç¯‡çŸ­æ–‡
4. T5çš„å¯¹è”ç”Ÿæˆæ•ˆæœå¥½äºGPT2ã€GPT2çš„è¯—è¯ç”Ÿæˆæ•ˆæœå¥½äºT5

- [å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)
- [å¤è¯—ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%8F%A4%E8%AF%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)

## TGLS æ¨¡å‹ï¼ˆæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼‰

æ— ç›‘ç£çš„ä¸­æ–‡ç”µå•†è¯„è®ºç”Ÿæˆï¼šä»**ç”µå•†è¯„è®º**ä¸­æå–ç”¨æˆ·è¡¨è¾¾è§‚ç‚¹çš„çŸ­å¥å¹¶è¿›è¡Œç»„åˆæ¥ç”Ÿæˆä»¿çœŸè¯„è®ºã€‚

example: [examples/unsup_generation_demo.py](examples/unsup_generation_demo.py)

```python
import os
import sys

sys.path.append('..')
from textgen.unsup_generation import TglsModel
from textgen.unsup_generation.phrase import load_list

pwd_path = os.path.abspath(os.path.dirname(__file__))

samples = load_list(os.path.join(pwd_path, './data/ecommerce_comments.txt'))
docs_text = [
    ["æŒºå¥½çš„ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä¹Ÿå¾ˆå®æƒ ï¼Œä¸çŸ¥æ•ˆæœå¦‚ä½•",
     "äº§å“æ²¡å¾—è¯´ï¼Œä¹°äº†ä»¥åå°±é™ä»·ï¼Œå¿ƒæƒ…ä¸ç¾ä¸½ã€‚",
     "åˆšæ”¶åˆ°ï¼ŒåŒ…è£…å¾ˆå®Œæ•´ï¼Œä¸é”™",
     "å‘è´§é€Ÿåº¦å¾ˆå¿«ï¼Œç‰©æµä¹Ÿä¸é”™ï¼ŒåŒä¸€æ—¶é—´ä¹°çš„ä¸¤ä¸ªä¸œä¸œï¼Œä¸€ä¸ªå…ˆåˆ°ä¸€ä¸ªè¿˜åœ¨è·¯ä¸Šã€‚è¿™ä¸ªæ°´æ°´å¾ˆå–œæ¬¢ï¼Œä¸è¿‡ç›–å­çœŸçš„å¼€äº†ã€‚ç›–ä¸ç‰¢äº†ç°åœ¨ã€‚",
     "åŒ…è£…çš„å¾ˆå¥½ï¼Œæ˜¯æ­£å“",
     "è¢«ç§è‰å…°è”»ç²‰æ°´ä¸‰ç™¾å…ƒä¸€å¤§ç“¶å›¤è´§ï¼Œå¸Œæœ›æ˜¯æ­£å“å¥½ç”¨ï¼Œæ”¶åˆ°çš„æ—¶å€™ç”¨ä¿é²œè†œåŒ…è£¹å¾—ä¸¥ä¸¥å®å®ï¼Œåªæ•¢ä¹°è€ƒæ‹‰è‡ªè¥çš„æŠ¤è‚¤å“",
     ],
    ['å¾ˆæ¸©å’Œï¼Œæ¸…æ´—çš„ä¹Ÿå¾ˆå¹²å‡€ï¼Œä¸æ²¹è…»ï¼Œå¾ˆä¸é”™ï¼Œä¼šè€ƒè™‘å›è´­ï¼Œç¬¬ä¸€æ¬¡è€ƒæ‹‰ä¹°æŠ¤è‚¤å“ï¼Œæ»¡æ„',
     'è¿™æ¬¾å¸å¦†æ²¹æˆ‘ä¼šæ— é™å›è´­çš„ã€‚å³ä½¿æˆ‘æ˜¯æ²¹ç—˜çš®ï¼Œä¹Ÿä¸ä¼šé—·ç—˜ï¼ŒåŒæ—¶åœ¨è„¸éƒ¨æŒ‰æ‘©æ—¶ï¼Œè¿˜èƒ½è§£å†³ç™½å¤´çš„è„‚è‚ªç²’çš„é—®é¢˜ã€‚ç”¨æ¸…æ°´æ´—å®Œè„¸åï¼Œéå¸¸çš„æ¸…çˆ½ã€‚',
     'è‡ªä»ç”¨äº†fanclä¹‹åå°±ä¸ç”¨å…¶ä»–å¸å¦†äº†ï¼Œå¸çš„èˆ’æœåˆå¹²å‡€',
     'ä¹°è´µäº†ï¼Œå¤§æ¶¦å‘æ‰å–79ã€‚9ã€‚',
     ],
    samples
]
m = TglsModel(docs_text)
r = m.generate(samples[:500])
print('size:', len(r))
for review in r:
    print('\t' + review)
```

output:

[ç¾è¿ªæƒ å°” N.M.Fé’ˆå‰‚æ°´åº“ä¿æ¹¿é¢è†œ](https://goods.kaola.com/product/2227311.html)æœ‰å¦‚ä¸‹çš„20å¥è¯„è®ºï¼Œå…¶ä¸­æœ‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œ10å¥æ˜¯ç”Ÿæˆçš„è¯„è®ºï¼Œèƒ½çœ‹å‡ºæ¥ä¹ˆ?ğŸ˜‚

```
è¿˜ä¸é”™è¿˜ä¸é”™è¿˜ä¸é”™è¿˜ä¸é”™ã€‚
ä¸œè¥¿åˆ°äº†ï¼Œä¸çŸ¥é“å¥½ä¸å¥½ç”¨ã€‚è¯•ç”¨è¿‡åå†æ¥è¯„ä»·ã€‚åˆ°æ—¶çœ‹ç½‘è¯„éƒ½è¿˜å¯ä»¥ã€‚
å“ºä¹³æœŸå”¯ä¸€ä½¿ç”¨çš„æŠ¤è‚¤å“ï¼Œæ¯å¤©éƒ½æ˜¯ç´ é¢œï¼Œè„¸é¢å…¨é é¢è†œåŠç€ğŸ˜„è¡¥æ°´ğŸ’¦ä¸ç²˜è…»ä¸€å¦‚æ—¢å¾€çš„æ”¯æŒï¼Œå–œæ¬¢ğŸ’•
ææ´»åŠ¨æ—¶ä¹°çš„é¢è†œï¼Œä¸çŸ¥é“è¿™ä¸ªé¢è†œæ˜¯çœŸæ˜¯å‡æ•·åœ¨è„¸ä¸Šé¢è†œçº¸éƒ½æœ‰å°æ°´æ³¡é¼“èµ·æ¥ã€‚
å¾ˆä¸é”™ï¼Œéå¸¸è¡¥æ°´ï¼Œç”¨è¿‡çš„éƒ½çŸ¥é“ï¼Œæ€§ä»·æ¯”ä¹‹ç‹ï¼Œå¥½ç”¨åˆä¸è´µï¼Œæ­£å“ï¼Œç”¨ç€æ”¾å¿ƒï¼Œç‰©æµä¹Ÿå¾ˆå¿«ã€‚
é¢è†œéå¸¸å¥½ç”¨å“¦ã€‚é¢è†œè–„è–„çš„ã€‚å¥½åƒæ˜¯èš•ä¸é¢è†œå•Šã€‚ç²¾åå¾ˆå¤šå‘¢ã€‚æ•·åœ¨è„¸ä¸Šå¾ˆèˆ’æœã€‚æ„Ÿè§‰æŒºä¿æ¹¿çš„ï¼Œå‘³é“ä¹ŸæŒºå¥½é—»çš„ã€‚å°±æ˜¯é‡Œé¢åªæœ‰å•çº¯çš„é¢è†œç›´æ¥æ•·è„¸ä¸Šæœ‰ç‚¹ä¸å¥½å¼„ï¼Œå“ˆå“ˆå“ˆ
è¿˜å¯ä»¥ä¿æ¹¿æ•ˆæœä¸é”™æ°´æ¶¦æ¶¦çš„æ¯å¤©è´´ä¸€ç‰‡è„¸ä¹Ÿä¸å¹²äº†ç”¨å®Œäº†åœ¨ä¹°ç‚¹ï¼Œä¸é”™è¿˜ä¼šç»§ç»­å›è´­çš„ã€‚
å¿«é€’å¾ˆå¿«ï¼Œä¸œè¥¿å¾ˆèµï¼æƒ³è¦å¾—ç‚¹è€ƒæ‹‰è±†ä¸å®¹æ˜“ï¼Œè¿˜è¦ä¸‰åä¸ªå­—ã€‚æ—¶é—´å®è´µï¼ŒåºŸè¯ä¸è¯´ï¼ç”¨è¿‡äº†å°±çŸ¥é“äº†
æŒºå¥½ç”¨çš„ï¼Œæœ‹å‹æ¨èæ¥çš„
æŒºå¥½ç”¨çš„ï¼Œæ·¡æ·¡çš„ï¼Œè™½ç„¶ä¸æ˜¯å¾ˆæµ“ç²¾åçš„æ„Ÿè§‰ï¼Œä½†æ˜¯æ•ˆæœä¹Ÿè›®å¥½çš„ã€‚åˆ’ç®—
ä¸å¾—ä¸è¯´ç¾è¿ªæƒ å°”çš„é¢è†œæ˜¯æˆ‘ç”¨è¿‡çš„æœ€å¥½çš„é¢è†œä¹‹ä¸€ğŸ˜è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œæ²¡æƒ³åˆ°è¿™ä¹ˆä¾¿å®œçš„ä»·æ ¼ç«ŸçœŸçš„èƒ½ä¹°åˆ°çœŸå“ã€‚
ä¿æ¹¿æ•ˆæœæŒºå¥½çš„ï¼Œé¢è†œå¾ˆå¥½ç”¨ã€‚
æœŸå¾…å¥½çš„äº§å“ã€‚
ä¸€æ‰“å¼€åŒ…è£…é‡Œé¢çš„ç²¾ååˆšåˆšå¥½ï¼Œç”¨äº†è¡¥æ°´è¡¥æ°´æ•ˆæœä¸é”™ï¼Œç‰©æµéå¸¸å¿«ã€‚
çš®è‚¤å¾ˆå…‰æ»‘ğŸ˜‡æ¯”ä¸Šå»é€Ÿåº¦å¿«ä¸‰å¤©å°±åˆ°äº†ã€‚
å‰ä¸¤å¤©çš®è‚¤å¹²ç‡¥è¿ç»­æ•·äº†ä¸¤ä¸ªæ™šä¸Šæ„Ÿè§‰è¿˜ä¸é”™ğŸ˜‚è¡¥æ°´æ•ˆæœæ˜æ˜¾ï¼å¯æƒ³è€ŒçŸ¥ç²¾åæ¶²åˆå¤šå……è¶³ğŸ˜æ•·ä¸Šä»¥åå‡‰å‡‰çš„å¾ˆèˆ’æœã€‚
è¡¥æ°´æ•ˆæœä¸€èˆ¬å§ï½ä½†æ˜¯æˆ‘ç”¨çš„éŸ©å›½èƒŒå›æ¥çš„é¢è†œçº¸ä¸ç®—è–„ï¼Œå¸Œæœ›å¥½ç”¨ä¼šå›è´­çš„ï¼Œæ•·ä¸Šè„¸æ„Ÿè§‰æ¯”è¾ƒæ¸…çˆ½ï½ä»·æ ¼è¿˜ä¸ä¾¿å®œã€‚
å¸Œæœ›å¥½ç”¨ï¼Œé¢è†œç”¨è¿‡äº†å¾ˆå¥½ç”¨ï¼Œçš®è‚¤æ°´å«©å…‰æ»‘ç™½çš™ï¼Œè¡¥æ°´ä¸é”™ï¼Œä»·æ ¼ä¹Ÿåˆé€‚ã€‚
å°±æ˜¯ç²¾åæ¶²å¤ªå°‘äº†ï¼Œä¿æ¹¿æ•ˆæœä¸é”™ã€‚
é¢è†œçš„è¡¥æ°´æ•ˆæœéå¸¸å¥½ï¼Œä¿æ¹¿æ•ˆæœç¡®å®å¾ˆèµï¼Œè¿™ä¸ªé¢è†œç›¸å¯¹äºèƒ¶åŸè›‹ç™½å’Œç¾ç™½çš„é‚£ä¸¤æ¬¾çš„é¢è†œçº¸è¦åšä¸€äº›ï¼Œçœ‹ç€ä»·æ ¼åˆé€‚ã€‚
```

å‰10å¥æ˜¯çœŸå®ç”¨æˆ·è¯„è®ºï¼Œå10å¥æ˜¯ç”Ÿæˆçš„ã€‚

# Contact

- Issue(å»ºè®®)
  ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="docs/wechat.jpeg" width="200" />

# License

æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚

# Contribute

é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

- åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
- ä½¿ç”¨`python -m pytest`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

## Reference

- [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
- [minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn)
- [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)
- [asyml/texar](https://github.com/asyml/texar)
- [yangjianxin1/GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat)
- [williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch)
- [RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)
- [Tiiiger/bert_score]()
- [1YCxZ/Fake-review-generation](https://github.com/1YCxZ/Fake-review-generation)
